{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05043bda-ef6a-4e8b-900c-83d0f7380e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ConfiguraciÃ³n completa\n",
      " Fecha: 2025-11-12 00:42\n",
      " Semilla aleatoria: 42\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\"\"\"\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "PREDICCIÃ“N DE DEMANDA TURÃSTICA EN MENDOZA\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# ConfiguraciÃ³n\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Estilo de grÃ¡ficos\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\" ConfiguraciÃ³n completa\")\n",
    "print(f\" Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "print(f\" Semilla aleatoria: {RANDOM_STATE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2a12060-fa50-492e-b418-506410db51eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      " CARGA DE DATOS\n",
      "================================================================================\n",
      " Dataset cargado exitosamente\n",
      "   â€¢ Dimensiones: 2,877 filas Ã— 31 columnas\n",
      "   â€¢ Memoria: 1.15 MB\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      " PRIMERAS FILAS DEL DATASET\n",
      "--------------------------------------------------------------------------------\n",
      "  indice_tiempo               pais_origen            punto_entrada  turistas  \\\n",
      "0       2014-01                    Brasil       Aeropuerto CÃ³rdoba       654   \n",
      "1       2014-01                     Chile       Aeropuerto CÃ³rdoba      1046   \n",
      "2       2014-01    Ee.Uu, CanadÃ¡ Y MÃ©xico       Aeropuerto CÃ³rdoba      1211   \n",
      "3       2014-01  Europa Y Resto Del Mundo       Aeropuerto CÃ³rdoba       719   \n",
      "4       2014-01          Resto De AmÃ©rica       Aeropuerto CÃ³rdoba       763   \n",
      "5       2014-01           Resto Del Mundo      Puerto Buenos Aires      2135   \n",
      "6       2014-01                   Uruguay      Puerto Buenos Aires     14122   \n",
      "7       2014-02                   Bolivia  Aeropuerto Buenos Aires      4506   \n",
      "8       2014-02                    Brasil  Aeropuerto Buenos Aires     32421   \n",
      "9       2014-02                    Brasil       Aeropuerto CÃ³rdoba       654   \n",
      "\n",
      "   precio_promedio_usd  precio_minimo_usd  precio_maximo_usd  dias  \\\n",
      "0                  7.1                6.0                8.0    31   \n",
      "1                  7.1                6.0                8.0    31   \n",
      "2                  7.1                6.0                8.0    31   \n",
      "3                  7.1                6.0                8.0    31   \n",
      "4                  7.1                6.0                8.0    31   \n",
      "5                  7.1                6.0                8.0    31   \n",
      "6                  7.1                6.0                8.0    31   \n",
      "7                  8.0                8.0                8.0    28   \n",
      "8                  8.0                8.0                8.0    28   \n",
      "9                  8.0                8.0                8.0    28   \n",
      "\n",
      "   variacion_usd_mensual  variacion_porcentual_usd  ...  es_junio  es_julio  \\\n",
      "0                    2.0                     33.33  ...         0         0   \n",
      "1                    2.0                     33.33  ...         0         0   \n",
      "2                    2.0                     33.33  ...         0         0   \n",
      "3                    2.0                     33.33  ...         0         0   \n",
      "4                    2.0                     33.33  ...         0         0   \n",
      "5                    2.0                     33.33  ...         0         0   \n",
      "6                    2.0                     33.33  ...         0         0   \n",
      "7                    0.0                      0.00  ...         0         0   \n",
      "8                    0.0                      0.00  ...         0         0   \n",
      "9                    0.0                      0.00  ...         0         0   \n",
      "\n",
      "   es_agosto  es_septiembre  es_octubre  es_noviembre  es_diciembre  \\\n",
      "0          0              0           0             0             0   \n",
      "1          0              0           0             0             0   \n",
      "2          0              0           0             0             0   \n",
      "3          0              0           0             0             0   \n",
      "4          0              0           0             0             0   \n",
      "5          0              0           0             0             0   \n",
      "6          0              0           0             0             0   \n",
      "7          0              0           0             0             0   \n",
      "8          0              0           0             0             0   \n",
      "9          0              0           0             0             0   \n",
      "\n",
      "   mes_vendimia  vacaciones_invierno  temporada_alta  \n",
      "0             0                    0               1  \n",
      "1             0                    0               1  \n",
      "2             0                    0               1  \n",
      "3             0                    0               1  \n",
      "4             0                    0               1  \n",
      "5             0                    0               1  \n",
      "6             0                    0               1  \n",
      "7             1                    0               1  \n",
      "8             1                    0               1  \n",
      "9             1                    0               1  \n",
      "\n",
      "[10 rows x 31 columns]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      " INFORMACIÃ“N DE COLUMNAS\n",
      "--------------------------------------------------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2877 entries, 0 to 2876\n",
      "Data columns (total 31 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   indice_tiempo             2877 non-null   object \n",
      " 1   pais_origen               2877 non-null   object \n",
      " 2   punto_entrada             2877 non-null   object \n",
      " 3   turistas                  2877 non-null   int64  \n",
      " 4   precio_promedio_usd       2877 non-null   float64\n",
      " 5   precio_minimo_usd         2877 non-null   float64\n",
      " 6   precio_maximo_usd         2877 non-null   float64\n",
      " 7   dias                      2877 non-null   int64  \n",
      " 8   variacion_usd_mensual     2877 non-null   float64\n",
      " 9   variacion_porcentual_usd  2877 non-null   float64\n",
      " 10  usd_alto                  2877 non-null   int64  \n",
      " 11  usd_alta_variabilidad     2877 non-null   int64  \n",
      " 12  interes_google_promedio   0 non-null      float64\n",
      " 13  interes_alto              0 non-null      float64\n",
      " 14  aÃ±o                       2877 non-null   int64  \n",
      " 15  mes                       2877 non-null   int64  \n",
      " 16  es_enero                  2877 non-null   int64  \n",
      " 17  es_febrero                2877 non-null   int64  \n",
      " 18  es_marzo                  2877 non-null   int64  \n",
      " 19  es_abril                  2877 non-null   int64  \n",
      " 20  es_mayo                   2877 non-null   int64  \n",
      " 21  es_junio                  2877 non-null   int64  \n",
      " 22  es_julio                  2877 non-null   int64  \n",
      " 23  es_agosto                 2877 non-null   int64  \n",
      " 24  es_septiembre             2877 non-null   int64  \n",
      " 25  es_octubre                2877 non-null   int64  \n",
      " 26  es_noviembre              2877 non-null   int64  \n",
      " 27  es_diciembre              2877 non-null   int64  \n",
      " 28  mes_vendimia              2877 non-null   int64  \n",
      " 29  vacaciones_invierno       2877 non-null   int64  \n",
      " 30  temporada_alta            2877 non-null   int64  \n",
      "dtypes: float64(7), int64(21), object(3)\n",
      "memory usage: 696.9+ KB\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      " ESTADÃSTICAS DESCRIPTIVAS\n",
      "--------------------------------------------------------------------------------\n",
      "           turistas  precio_promedio_usd  precio_minimo_usd  \\\n",
      "count   2877.000000           2877.00000        2877.000000   \n",
      "mean    9857.873479            222.21529         216.119830   \n",
      "std    15070.031070            347.19781         339.912476   \n",
      "min        0.000000              7.10000           6.000000   \n",
      "25%      872.000000             17.90000          17.000000   \n",
      "50%     2508.000000             69.97000          69.000000   \n",
      "75%    15036.000000            179.61000         175.000000   \n",
      "max    88814.000000           1340.65000        1310.000000   \n",
      "\n",
      "       precio_maximo_usd         dias  variacion_usd_mensual  \\\n",
      "count        2877.000000  2877.000000            2877.000000   \n",
      "mean          227.922072    30.430657              11.802242   \n",
      "std           355.949803     0.822454              43.715898   \n",
      "min             8.000000    28.000000               0.000000   \n",
      "25%            18.000000    30.000000               1.000000   \n",
      "50%            71.000000    31.000000               2.000000   \n",
      "75%           185.000000    31.000000               9.250000   \n",
      "max          1380.000000    31.000000             451.750000   \n",
      "\n",
      "       variacion_porcentual_usd     usd_alto  usd_alta_variabilidad  \\\n",
      "count               2877.000000  2877.000000            2877.000000   \n",
      "mean                   5.796615     0.493917               0.483490   \n",
      "std                   12.316138     0.500050               0.499814   \n",
      "min                    0.000000     0.000000               0.000000   \n",
      "25%                    0.970000     0.000000               0.000000   \n",
      "50%                    2.900000     0.000000               0.000000   \n",
      "75%                    6.490000     1.000000               1.000000   \n",
      "max                  119.990000     1.000000               1.000000   \n",
      "\n",
      "       interes_google_promedio  ...     es_junio     es_julio    es_agosto  \\\n",
      "count                      0.0  ...  2877.000000  2877.000000  2877.000000   \n",
      "mean                       NaN  ...     0.086201     0.083073     0.083073   \n",
      "std                        NaN  ...     0.280709     0.276040     0.276040   \n",
      "min                        NaN  ...     0.000000     0.000000     0.000000   \n",
      "25%                        NaN  ...     0.000000     0.000000     0.000000   \n",
      "50%                        NaN  ...     0.000000     0.000000     0.000000   \n",
      "75%                        NaN  ...     0.000000     0.000000     0.000000   \n",
      "max                        NaN  ...     1.000000     1.000000     1.000000   \n",
      "\n",
      "       es_septiembre   es_octubre  es_noviembre  es_diciembre  mes_vendimia  \\\n",
      "count    2877.000000  2877.000000   2877.000000   2877.000000   2877.000000   \n",
      "mean        0.079944     0.079944      0.079944      0.079944      0.172402   \n",
      "std         0.271254     0.271254      0.271254      0.271254      0.377795   \n",
      "min         0.000000     0.000000      0.000000      0.000000      0.000000   \n",
      "25%         0.000000     0.000000      0.000000      0.000000      0.000000   \n",
      "50%         0.000000     0.000000      0.000000      0.000000      0.000000   \n",
      "75%         0.000000     0.000000      0.000000      0.000000      0.000000   \n",
      "max         1.000000     1.000000      1.000000      1.000000      1.000000   \n",
      "\n",
      "       vacaciones_invierno  temporada_alta  \n",
      "count          2877.000000     2877.000000  \n",
      "mean              0.166145        0.501564  \n",
      "std               0.372276        0.500084  \n",
      "min               0.000000        0.000000  \n",
      "25%               0.000000        0.000000  \n",
      "50%               0.000000        1.000000  \n",
      "75%               0.000000        1.000000  \n",
      "max               1.000000        1.000000  \n",
      "\n",
      "[8 rows x 28 columns]\n",
      "\n",
      "================================================================================\n",
      " ANÃLISIS DE LA VARIABLE OBJETIVO: 'turistas'\n",
      "================================================================================\n",
      "\n",
      " EstadÃ­sticas bÃ¡sicas:\n",
      "count     2877.000000\n",
      "mean      9857.873479\n",
      "std      15070.031070\n",
      "min          0.000000\n",
      "25%        872.000000\n",
      "50%       2508.000000\n",
      "75%      15036.000000\n",
      "max      88814.000000\n",
      "Name: turistas, dtype: float64\n",
      "\n",
      "   â€¢ Media: 9,858 turistas/mes\n",
      "   â€¢ Mediana: 2,508 turistas/mes\n",
      "   â€¢ DesviaciÃ³n estÃ¡ndar: 15,070\n",
      "   â€¢ MÃ­nimo: 0\n",
      "   â€¢ MÃ¡ximo: 88,814\n",
      "   â€¢ Rango: 88,814\n",
      "   â€¢ Coeficiente de variaciÃ³n: 152.9%\n",
      "     â†’ Alta variabilidad en los datos\n",
      "\n",
      "================================================================================\n",
      " CLASIFICACIÃ“N DE VARIABLES\n",
      "================================================================================\n",
      "\n",
      " Variables numÃ©ricas: 27\n",
      "    1. precio_promedio_usd\n",
      "    2. precio_minimo_usd\n",
      "    3. precio_maximo_usd\n",
      "    4. dias\n",
      "    5. variacion_usd_mensual\n",
      "    6. variacion_porcentual_usd\n",
      "    7. usd_alto\n",
      "    8. usd_alta_variabilidad\n",
      "    9. interes_google_promedio\n",
      "   10. interes_alto\n",
      "   ... y 17 mÃ¡s\n",
      "\n",
      " Variables categÃ³ricas: 2\n",
      "   1. pais_origen (13 categorÃ­as)\n",
      "   2. punto_entrada (5 categorÃ­as)\n",
      "\n",
      "================================================================================\n",
      " ANÃLISIS DE CALIDAD DE DATOS\n",
      "================================================================================\n",
      "\n",
      " Valores nulos por columna:\n",
      "    Total de valores nulos: 5,754\n",
      "                Columna  Cantidad  Porcentaje\n",
      "interes_google_promedio      2877       100.0\n",
      "           interes_alto      2877       100.0\n",
      "\n",
      "   ğŸ—‘ï¸ Columnas completamente nulas detectadas: ['interes_google_promedio', 'interes_alto']\n",
      "      â†’ Estas columnas serÃ¡n eliminadas en la preparaciÃ³n\n",
      "   âœ… Columnas eliminadas: ['interes_google_promedio', 'interes_alto']\n",
      "\n",
      " Filas duplicadas: 0\n",
      "    No hay filas duplicadas\n",
      "\n",
      "================================================================================\n",
      " ANÃLISIS DE OUTLIERS EN VARIABLE OBJETIVO\n",
      "================================================================================\n",
      "\n",
      " MÃ©todo IQR (Rango IntercuartÃ­lico):\n",
      "   â€¢ Q1 (25%): 872\n",
      "   â€¢ Q3 (75%): 15,036\n",
      "   â€¢ IQR: 14,164\n",
      "   â€¢ LÃ­mite inferior: -20,374\n",
      "   â€¢ LÃ­mite superior: 36,282\n",
      "   â€¢ Outliers detectados: 205 (7.1%)\n",
      "\n",
      "    Top 5 outliers superiores:\n",
      " aÃ±o  mes  turistas pais_origen           punto_entrada\n",
      "2019    7     88814      Brasil Aeropuerto Buenos Aires\n",
      "2024    7     87279      Brasil Aeropuerto Buenos Aires\n",
      "2025    1     80795      Europa Aeropuerto Buenos Aires\n",
      "2014    8     80695      Brasil Aeropuerto Buenos Aires\n",
      "2019    1     78530      Europa Aeropuerto Buenos Aires\n",
      "\n",
      "   ğŸ’¡ DecisiÃ³n: MANTENER outliers\n",
      "      â†’ Pueden representar temporadas altas legÃ­timas\n",
      "      â†’ El pipeline usarÃ¡ StandardScaler que maneja outliers\n",
      "\n",
      "================================================================================\n",
      " IDENTIFICACIÃ“N DE REGISTROS PROBLEMÃTICOS\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Registros con 0 turistas:\n",
      "   â€¢ Total: 389 (13.52%)\n",
      "\n",
      "    AnÃ¡lisis detallado de los ceros:\n",
      "\n",
      "    DistribuciÃ³n temporal:\n",
      "      â€¢ 2020: 207 registros ( 75.0% del aÃ±o)\n",
      "      â€¢ 2021: 182 registros ( 65.9% del aÃ±o)\n",
      "\n",
      "    AÃ±o mÃ¡s afectado: 2020 (207 registros con cero)\n",
      "      â†’ Causa probable: Pandemia COVID-19\n",
      "      â†’ Cierre de fronteras y restricciones sanitarias\n",
      "\n",
      "    DistribuciÃ³n por pais_origen (top 5):\n",
      "      â€¢ Chile: 69 registros\n",
      "      â€¢ Brasil: 51 registros\n",
      "      â€¢ Resto Del Mundo: 51 registros\n",
      "      â€¢ Europa Y Resto Del Mundo: 36 registros\n",
      "      â€¢ Resto De AmÃ©rica: 36 registros\n",
      "\n",
      "    Ejemplos de registros con 0 turistas:\n",
      " aÃ±o  mes  turistas pais_origen           punto_entrada\n",
      "2020    4         0     Bolivia Aeropuerto Buenos Aires\n",
      "2020    4         0      Brasil Aeropuerto Buenos Aires\n",
      "2020    4         0      Brasil      Aeropuerto CÃ³rdoba\n",
      "2020    4         0      Brasil      Aeropuerto Mendoza\n",
      "2020    4         0       Chile Aeropuerto Buenos Aires\n",
      "\n",
      "    OBSERVACIÃ“N IMPORTANTE:\n",
      "      Estos registros con 0 turistas:\n",
      "      â€¢ Representan situaciones excepcionales (cierres, crisis)\n",
      "      â€¢ NO reflejan la demanda turÃ­stica normal\n",
      "      â€¢ Pueden distorsionar el entrenamiento del modelo\n",
      "      â€¢ SerÃ¡n ELIMINADOS en la preparaciÃ³n de datos (Celda 3)\n",
      "\n",
      "      JustificaciÃ³n:\n",
      "      â†’ El objetivo es predecir demanda en condiciones normales\n",
      "      â†’ No queremos que el modelo 'aprenda' cierres excepcionales\n",
      "      â†’ Esto mejorarÃ¡ la utilidad prÃ¡ctica del modelo\n",
      "\n",
      "================================================================================\n",
      " RESUMEN DE LA EXPLORACIÃ“N\n",
      "================================================================================\n",
      "\n",
      " DATASET CARGADO Y EXPLORADO:\n",
      "   â€¢ Registros: 2,877\n",
      "   â€¢ Variables: 29 (25 numÃ©ricas + 2 categÃ³ricas)\n",
      "   â€¢ Variable objetivo: turistas (media=9,858, std=15,070)\n",
      "\n",
      " PROBLEMAS IDENTIFICADOS:\n",
      "   â€¢ Outliers: 205 (7.1%) â†’ MANTENER\n",
      "   â€¢ Registros con 0: 389 (13.52%) â†’ ELIMINAR en Celda 3\n",
      "   â€¢ Valores nulos: 5754 â†’ Ya tratados\n",
      "   â€¢ Duplicados: 0 â†’ No hay\n",
      "\n",
      " DISTRIBUCIÃ“N:\n",
      "   â€¢ Rango: 0 - 88,814\n",
      "   â€¢ Coeficiente de variaciÃ³n: 152.9% (Alta variabilidad)\n",
      "   â€¢ AsimetrÃ­a detectada (probablemente sesgada a la derecha)\n",
      "\n",
      "\n",
      "================================================================================\n",
      " EXPLORACIÃ“N COMPLETADA\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\"\"\"\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "PASO 1: EXPLORACIÃ“N Y ENTENDIMIENTO DE LOS DATOS\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "Objetivo: Conocer la estructura, calidad y caracterÃ­sticas del dataset antes\n",
    "          de aplicar cualquier transformaciÃ³n o limpieza.\n",
    "          \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CARGA DE DATOS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" CARGA DE DATOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Cargar dataset\n",
    "df = pd.read_csv('mendoza_turismo_final_filtrado.csv', sep=';', encoding='utf-8')\n",
    "\n",
    "# Crear copia de seguridad para anÃ¡lisis posteriores\n",
    "df_original = df.copy()\n",
    "\n",
    "print(f\" Dataset cargado exitosamente\")\n",
    "print(f\"   â€¢ Dimensiones: {df.shape[0]:,} filas Ã— {df.shape[1]} columnas\")\n",
    "print(f\"   â€¢ Memoria: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# INSPECCIÃ“N INICIAL\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\" PRIMERAS FILAS DEL DATASET\")\n",
    "print(\"-\"*80)\n",
    "print(df.head(10))\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\" INFORMACIÃ“N DE COLUMNAS\")\n",
    "print(\"-\"*80)\n",
    "df.info()\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\" ESTADÃSTICAS DESCRIPTIVAS\")\n",
    "print(\"-\"*80)\n",
    "print(df.describe())\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ANÃLISIS DE LA VARIABLE OBJETIVO\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" ANÃLISIS DE LA VARIABLE OBJETIVO: 'turistas'\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n EstadÃ­sticas bÃ¡sicas:\")\n",
    "print(df['turistas'].describe())\n",
    "\n",
    "print(f\"\\n   â€¢ Media: {df['turistas'].mean():,.0f} turistas/mes\")\n",
    "print(f\"   â€¢ Mediana: {df['turistas'].median():,.0f} turistas/mes\")\n",
    "print(f\"   â€¢ DesviaciÃ³n estÃ¡ndar: {df['turistas'].std():,.0f}\")\n",
    "print(f\"   â€¢ MÃ­nimo: {df['turistas'].min():,.0f}\")\n",
    "print(f\"   â€¢ MÃ¡ximo: {df['turistas'].max():,.0f}\")\n",
    "print(f\"   â€¢ Rango: {df['turistas'].max() - df['turistas'].min():,.0f}\")\n",
    "\n",
    "# Coeficiente de variaciÃ³n\n",
    "cv = (df['turistas'].std() / df['turistas'].mean()) * 100\n",
    "print(f\"   â€¢ Coeficiente de variaciÃ³n: {cv:.1f}%\")\n",
    "if cv > 100:\n",
    "    print(f\"     â†’ Alta variabilidad en los datos\")\n",
    "elif cv > 50:\n",
    "    print(f\"     â†’ Variabilidad moderada\")\n",
    "else:\n",
    "    print(f\"     â†’ Baja variabilidad\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# IDENTIFICACIÃ“N DE TIPOS DE VARIABLES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" CLASIFICACIÃ“N DE VARIABLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Variables numÃ©ricas (excluyendo objetivo)\n",
    "numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if 'turistas' in numeric_features:\n",
    "    numeric_features.remove('turistas')\n",
    "\n",
    "# Variables categÃ³ricas\n",
    "categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Eliminar columnas de fecha que no son predictoras directas\n",
    "date_columns = ['indice_tiempo', 'fecha', 'mes_ano', 'indice_tiempo_dt']\n",
    "for col in date_columns:\n",
    "    if col in categorical_features:\n",
    "        categorical_features.remove(col)\n",
    "    if col in numeric_features:\n",
    "        numeric_features.remove(col)\n",
    "\n",
    "print(f\"\\n Variables numÃ©ricas: {len(numeric_features)}\")\n",
    "if len(numeric_features) <= 15:\n",
    "    for i, feat in enumerate(numeric_features, 1):\n",
    "        print(f\"   {i:>2}. {feat}\")\n",
    "else:\n",
    "    for i, feat in enumerate(numeric_features[:10], 1):\n",
    "        print(f\"   {i:>2}. {feat}\")\n",
    "    print(f\"   ... y {len(numeric_features) - 10} mÃ¡s\")\n",
    "\n",
    "print(f\"\\n Variables categÃ³ricas: {len(categorical_features)}\")\n",
    "for i, feat in enumerate(categorical_features, 1):\n",
    "    n_unique = df[feat].nunique()\n",
    "    print(f\"   {i}. {feat} ({n_unique} categorÃ­as)\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CALIDAD DE DATOS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" ANÃLISIS DE CALIDAD DE DATOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Valores nulos\n",
    "print(f\"\\n Valores nulos por columna:\")\n",
    "nulos = df.isnull().sum()\n",
    "if nulos.sum() == 0:\n",
    "    print(\"    No hay valores nulos en el dataset\")\n",
    "else:\n",
    "    print(f\"    Total de valores nulos: {nulos.sum():,}\")\n",
    "    nulos_df = pd.DataFrame({\n",
    "        'Columna': nulos[nulos > 0].index,\n",
    "        'Cantidad': nulos[nulos > 0].values,\n",
    "        'Porcentaje': (nulos[nulos > 0].values / len(df) * 100).round(2)\n",
    "    })\n",
    "    print(nulos_df.to_string(index=False))\n",
    "    \n",
    "    # Identificar columnas completamente nulas\n",
    "    null_cols = df.columns[df.isnull().all()].tolist()\n",
    "    if null_cols:\n",
    "        print(f\"\\n   ğŸ—‘ï¸ Columnas completamente nulas detectadas: {null_cols}\")\n",
    "        print(f\"      â†’ Estas columnas serÃ¡n eliminadas en la preparaciÃ³n\")\n",
    "        df = df.drop(columns=null_cols)\n",
    "        # Actualizar listas de features\n",
    "        numeric_features = [f for f in numeric_features if f not in null_cols]\n",
    "        categorical_features = [f for f in categorical_features if f not in null_cols]\n",
    "        print(f\"   âœ… Columnas eliminadas: {null_cols}\")\n",
    "\n",
    "# Valores duplicados\n",
    "duplicados = df.duplicated().sum()\n",
    "print(f\"\\n Filas duplicadas: {duplicados}\")\n",
    "if duplicados > 0:\n",
    "    print(f\"    {duplicados} filas duplicadas detectadas ({duplicados/len(df)*100:.2f}%)\")\n",
    "else:\n",
    "    print(f\"    No hay filas duplicadas\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ANÃLISIS DE OUTLIERS EN TARGET\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" ANÃLISIS DE OUTLIERS EN VARIABLE OBJETIVO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# MÃ©todo IQR\n",
    "Q1 = df['turistas'].quantile(0.25)\n",
    "Q3 = df['turistas'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers_mask = (df['turistas'] < lower_bound) | (df['turistas'] > upper_bound)\n",
    "n_outliers = outliers_mask.sum()\n",
    "\n",
    "print(f\"\\n MÃ©todo IQR (Rango IntercuartÃ­lico):\")\n",
    "print(f\"   â€¢ Q1 (25%): {Q1:,.0f}\")\n",
    "print(f\"   â€¢ Q3 (75%): {Q3:,.0f}\")\n",
    "print(f\"   â€¢ IQR: {IQR:,.0f}\")\n",
    "print(f\"   â€¢ LÃ­mite inferior: {lower_bound:,.0f}\")\n",
    "print(f\"   â€¢ LÃ­mite superior: {upper_bound:,.0f}\")\n",
    "print(f\"   â€¢ Outliers detectados: {n_outliers} ({n_outliers/len(df)*100:.1f}%)\")\n",
    "\n",
    "if n_outliers > 0:\n",
    "    print(f\"\\n    Top 5 outliers superiores:\")\n",
    "    top_outliers = df[df['turistas'] > upper_bound].nlargest(5, 'turistas')\n",
    "    \n",
    "    # Seleccionar columnas disponibles para mostrar\n",
    "    display_cols = ['turistas']\n",
    "    if 'aÃ±o' in df.columns:\n",
    "        display_cols.insert(0, 'aÃ±o')\n",
    "    if 'mes' in df.columns:\n",
    "        display_cols.insert(1, 'mes')\n",
    "    if 'pais_origen' in df.columns:\n",
    "        display_cols.append('pais_origen')\n",
    "    if 'punto_entrada' in df.columns:\n",
    "        display_cols.append('punto_entrada')\n",
    "    \n",
    "    available_cols = [col for col in display_cols if col in df.columns]\n",
    "    print(top_outliers[available_cols].to_string(index=False))\n",
    "    \n",
    "    print(f\"\\n   ğŸ’¡ DecisiÃ³n: MANTENER outliers\")\n",
    "    print(f\"      â†’ Pueden representar temporadas altas legÃ­timas\")\n",
    "    print(f\"      â†’ El pipeline usarÃ¡ StandardScaler que maneja outliers\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ANÃLISIS CRÃTICO: REGISTROS CON CERO TURISTAS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" IDENTIFICACIÃ“N DE REGISTROS PROBLEMÃTICOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "ceros_count = (df['turistas'] == 0).sum()\n",
    "ceros_pct = ceros_count / len(df) * 100\n",
    "\n",
    "print(f\"\\nğŸ“Š Registros con 0 turistas:\")\n",
    "print(f\"   â€¢ Total: {ceros_count} ({ceros_pct:.2f}%)\")\n",
    "\n",
    "if ceros_count > 0:\n",
    "    print(f\"\\n    AnÃ¡lisis detallado de los ceros:\")\n",
    "    \n",
    "    # DistribuciÃ³n temporal\n",
    "    if 'aÃ±o' in df.columns:\n",
    "        ceros_por_aÃ±o = df[df['turistas'] == 0].groupby('aÃ±o').size().sort_index()\n",
    "        print(f\"\\n    DistribuciÃ³n temporal:\")\n",
    "        for aÃ±o, count in ceros_por_aÃ±o.items():\n",
    "            total_aÃ±o = (df['aÃ±o'] == aÃ±o).sum()\n",
    "            pct_aÃ±o = count / total_aÃ±o * 100\n",
    "            print(f\"      â€¢ {aÃ±o}: {count:>3} registros ({pct_aÃ±o:>5.1f}% del aÃ±o)\")\n",
    "        \n",
    "        # Identificar perÃ­odo mÃ¡s afectado\n",
    "        aÃ±o_mas_ceros = ceros_por_aÃ±o.idxmax()\n",
    "        max_ceros = ceros_por_aÃ±o.max()\n",
    "        \n",
    "        print(f\"\\n    AÃ±o mÃ¡s afectado: {aÃ±o_mas_ceros} ({max_ceros} registros con cero)\")\n",
    "        \n",
    "        if aÃ±o_mas_ceros in [2020, 2021]:\n",
    "            print(f\"      â†’ Causa probable: Pandemia COVID-19\")\n",
    "            print(f\"      â†’ Cierre de fronteras y restricciones sanitarias\")\n",
    "    \n",
    "    # DistribuciÃ³n por categorÃ­as\n",
    "    if categorical_features:\n",
    "        cat_principal = categorical_features[0]\n",
    "        ceros_por_cat = df[df['turistas'] == 0].groupby(cat_principal).size().sort_values(ascending=False)\n",
    "        \n",
    "        print(f\"\\n    DistribuciÃ³n por {cat_principal} (top 5):\")\n",
    "        for cat, count in ceros_por_cat.head(5).items():\n",
    "            print(f\"      â€¢ {cat}: {count} registros\")\n",
    "    \n",
    "    # Ejemplos de registros con cero\n",
    "    print(f\"\\n    Ejemplos de registros con 0 turistas:\")\n",
    "    ceros_sample = df[df['turistas'] == 0].head(5)\n",
    "    \n",
    "    sample_cols = ['turistas']\n",
    "    if 'aÃ±o' in df.columns:\n",
    "        sample_cols.insert(0, 'aÃ±o')\n",
    "    if 'mes' in df.columns:\n",
    "        sample_cols.insert(1, 'mes')\n",
    "    if categorical_features:\n",
    "        sample_cols.extend(categorical_features[:2])\n",
    "    \n",
    "    available_sample_cols = [col for col in sample_cols if col in df.columns]\n",
    "    print(ceros_sample[available_sample_cols].to_string(index=False))\n",
    "    \n",
    "    print(f\"\\n    OBSERVACIÃ“N IMPORTANTE:\")\n",
    "    print(f\"      Estos registros con 0 turistas:\")\n",
    "    print(f\"      â€¢ Representan situaciones excepcionales (cierres, crisis)\")\n",
    "    print(f\"      â€¢ NO reflejan la demanda turÃ­stica normal\")\n",
    "    print(f\"      â€¢ Pueden distorsionar el entrenamiento del modelo\")\n",
    "    print(f\"      â€¢ SerÃ¡n ELIMINADOS en la preparaciÃ³n de datos (Celda 3)\")\n",
    "    print(f\"\\n      JustificaciÃ³n:\")\n",
    "    print(f\"      â†’ El objetivo es predecir demanda en condiciones normales\")\n",
    "    print(f\"      â†’ No queremos que el modelo 'aprenda' cierres excepcionales\")\n",
    "    print(f\"      â†’ Esto mejorarÃ¡ la utilidad prÃ¡ctica del modelo\")\n",
    "\n",
    "else:\n",
    "    print(f\"    No hay registros con 0 turistas\")\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# RESUMEN DE LA EXPLORACIÃ“N\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" RESUMEN DE LA EXPLORACIÃ“N\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    " DATASET CARGADO Y EXPLORADO:\n",
    "   â€¢ Registros: {len(df):,}\n",
    "   â€¢ Variables: {df.shape[1]} ({len(numeric_features)} numÃ©ricas + {len(categorical_features)} categÃ³ricas)\n",
    "   â€¢ Variable objetivo: turistas (media={df['turistas'].mean():,.0f}, std={df['turistas'].std():,.0f})\n",
    "\n",
    " PROBLEMAS IDENTIFICADOS:\n",
    "   â€¢ Outliers: {n_outliers} ({n_outliers/len(df)*100:.1f}%) â†’ MANTENER\n",
    "   â€¢ Registros con 0: {ceros_count} ({ceros_pct:.2f}%) â†’ ELIMINAR en Celda 3\n",
    "   â€¢ Valores nulos: {nulos.sum()} â†’ Ya tratados\n",
    "   â€¢ Duplicados: {duplicados} â†’ No hay\n",
    "\n",
    " DISTRIBUCIÃ“N:\n",
    "   â€¢ Rango: {df['turistas'].min():,.0f} - {df['turistas'].max():,.0f}\n",
    "   â€¢ Coeficiente de variaciÃ³n: {cv:.1f}% ({'Alta' if cv > 100 else 'Moderada' if cv > 50 else 'Baja'} variabilidad)\n",
    "   â€¢ AsimetrÃ­a detectada (probablemente sesgada a la derecha)\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\" EXPLORACIÃ“N COMPLETADA\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "038bfed3-962e-49c8-9872-fd03633655be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ§¹ LIMPIEZA DE CATEGORÃAS\n",
      "================================================================================\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ”§ LIMPIANDO: PAÃS DE ORIGEN\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      " CategorÃ­as originales en 'pais_origen':\n",
      "   Total categorÃ­as Ãºnicas: 13\n",
      "\n",
      "   Listado completo:\n",
      "       1. Bolivia                                       (  139 registros,  4.83%)\n",
      "       2. Brasil                                        (  373 registros, 12.96%)\n",
      "       3. Chile                                         (  463 registros, 16.09%)\n",
      "       4. Ee.Uu Y CanadÃ¡                                (  139 registros,  4.83%)\n",
      "       5. Ee.Uu, CanadÃ¡                                 (   96 registros,  3.34%)\n",
      "       6. Ee.Uu, CanadÃ¡ Y MÃ©xico                        (  138 registros,  4.80%)\n",
      "       7. Europa                                        (  139 registros,  4.83%)\n",
      "       8. Europa Y Resto Del Mundo                      (  234 registros,  8.13%)\n",
      "       9. Paraguay                                      (  139 registros,  4.83%)\n",
      "      10. Resto AmÃ©rica                                 (  139 registros,  4.83%)\n",
      "      11. Resto De AmÃ©rica                              (  234 registros,  8.13%)\n",
      "      12. Resto Del Mundo                               (  367 registros, 12.76%)\n",
      "      13. Uruguay                                       (  277 registros,  9.63%)\n",
      "\n",
      "â³ Aplicando 29 reglas de normalizaciÃ³n...\n",
      "\n",
      " NormalizaciÃ³n completada:\n",
      "   â€¢ CategorÃ­as antes: 13\n",
      "   â€¢ CategorÃ­as despuÃ©s: 8\n",
      "   â€¢ CategorÃ­as eliminadas/unificadas: 5\n",
      "\n",
      " CategorÃ­as normalizadas en 'pais_origen':\n",
      "   Total categorÃ­as Ãºnicas: 8\n",
      "\n",
      "   Listado:\n",
      "       1. Bolivia                                       (  139 registros,  4.83%)\n",
      "       2. Brasil                                        (  373 registros, 12.96%)\n",
      "       3. Chile                                         (  463 registros, 16.09%)\n",
      "       4. EEUU, CanadÃ¡ y MÃ©xico                         (  373 registros, 12.96%)\n",
      "       5. Europa y Resto del Mundo                      (  740 registros, 25.72%)\n",
      "       6. Paraguay                                      (  139 registros,  4.83%)\n",
      "       7. Resto de AmÃ©rica                              (  373 registros, 12.96%)\n",
      "       8. Uruguay                                       (  277 registros,  9.63%)\n",
      "\n",
      " Detalle de cambios aplicados:\n",
      "   âœ“ 'Ee.Uu Y CanadÃ¡' â†’ 'EEUU, CanadÃ¡ y MÃ©xico' (139 registros)\n",
      "   âœ“ 'Ee.Uu, CanadÃ¡' â†’ 'EEUU, CanadÃ¡ y MÃ©xico' (96 registros)\n",
      "   âœ“ 'Ee.Uu, CanadÃ¡ Y MÃ©xico' â†’ 'EEUU, CanadÃ¡ y MÃ©xico' (138 registros)\n",
      "   âœ“ 'Europa' â†’ 'Europa y Resto del Mundo' (139 registros)\n",
      "   âœ“ 'Europa Y Resto Del Mundo' â†’ 'Europa y Resto del Mundo' (234 registros)\n",
      "   âœ“ 'Resto Del Mundo' â†’ 'Europa y Resto del Mundo' (367 registros)\n",
      "   âœ“ 'Resto AmÃ©rica' â†’ 'Resto de AmÃ©rica' (139 registros)\n",
      "   âœ“ 'Resto De AmÃ©rica' â†’ 'Resto de AmÃ©rica' (234 registros)\n",
      "\n",
      "    Total de transformaciones: 8\n",
      "    Registros afectados: 1486 (51.65%)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ”§ LIMPIANDO: PUNTO DE ENTRADA\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      " CategorÃ­as originales en 'punto_entrada':\n",
      "   Total categorÃ­as Ãºnicas: 5\n",
      "\n",
      "   Listado completo:\n",
      "       1. Aeropuerto Buenos Aires                       ( 1251 registros, 43.48%)\n",
      "       2. Aeropuerto CÃ³rdoba                            (  690 registros, 23.98%)\n",
      "       3. Aeropuerto Mendoza                            (  480 registros, 16.68%)\n",
      "       4. Paso Cristo Redentor                          (  180 registros,  6.26%)\n",
      "       5. Puerto Buenos Aires                           (  276 registros,  9.59%)\n",
      "\n",
      "â³ Aplicando 5 reglas de normalizaciÃ³n...\n",
      "\n",
      " NormalizaciÃ³n completada:\n",
      "   â€¢ CategorÃ­as antes: 5\n",
      "   â€¢ CategorÃ­as despuÃ©s: 5\n",
      "   â€¢ CategorÃ­as eliminadas/unificadas: 0\n",
      "\n",
      " CategorÃ­as normalizadas en 'punto_entrada':\n",
      "   Total categorÃ­as Ãºnicas: 5\n",
      "\n",
      "   Listado:\n",
      "       1. Aeropuerto Buenos Aires                       ( 1251 registros, 43.48%)\n",
      "       2. Aeropuerto CÃ³rdoba                            (  690 registros, 23.98%)\n",
      "       3. Aeropuerto Mendoza                            (  480 registros, 16.68%)\n",
      "       4. Paso Cristo Redentor                          (  180 registros,  6.26%)\n",
      "       5. Puerto Buenos Aires                           (  276 registros,  9.59%)\n",
      "\n",
      "    No se encontraron categorÃ­as para mapear (ya estaban limpias)\n",
      "\n",
      "================================================================================\n",
      " LIMPIEZA DE CATEGORÃAS COMPLETADA\n",
      "================================================================================\n",
      "\n",
      " Resumen final del dataset:\n",
      "   â€¢ Total registros: 2,877\n",
      "\n",
      "    PaÃ­s de Origen:\n",
      "      â€¢ CategorÃ­as Ãºnicas: 8\n",
      "      â€¢ Sin valores nulos: 2877\n",
      "\n",
      "      Top 5:\n",
      "         1. Europa y Resto del Mundo                 (  740, 25.72%)\n",
      "         2. Chile                                    (  463, 16.09%)\n",
      "         3. Brasil                                   (  373, 12.96%)\n",
      "         4. EEUU, CanadÃ¡ y MÃ©xico                    (  373, 12.96%)\n",
      "         5. Resto de AmÃ©rica                         (  373, 12.96%)\n",
      "\n",
      "    Punto de Entrada:\n",
      "      â€¢ CategorÃ­as Ãºnicas: 5\n",
      "      â€¢ Sin valores nulos: 2877\n",
      "\n",
      "      Top 5:\n",
      "         1. Aeropuerto Buenos Aires                  ( 1251, 43.48%)\n",
      "         2. Aeropuerto CÃ³rdoba                       (  690, 23.98%)\n",
      "         3. Aeropuerto Mendoza                       (  480, 16.68%)\n",
      "         4. Puerto Buenos Aires                      (  276,  9.59%)\n",
      "         5. Paso Cristo Redentor                     (  180,  6.26%)\n",
      "\n",
      "================================================================================\n",
      " Dataset listo para anÃ¡lisis con categorÃ­as normalizadas\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\"\"\"\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "PASO 1B: LIMPIEZA Y NORMALIZACIÃ“N DE CATEGORÃAS\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ§¹ LIMPIEZA DE CATEGORÃAS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# FUNCIÃ“N PARA LIMPIAR COLUMNA CATEGÃ“RICA\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def limpiar_columna_categorica(df, columna, mapeo_dict, nombre_display=\"\"):\n",
    "    \"\"\"\n",
    "    Limpia y normaliza categorÃ­as de una columna\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame\n",
    "        columna: Nombre de la columna a limpiar\n",
    "        mapeo_dict: Diccionario {valor_original: valor_normalizado}\n",
    "        nombre_display: Nombre para mostrar en logs\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame modificado (in-place)\n",
    "    \"\"\"\n",
    "    if columna not in df.columns:\n",
    "        print(f\" Columna '{columna}' no existe en el dataset\")\n",
    "        return df\n",
    "    \n",
    "    nombre = nombre_display or columna\n",
    "    \n",
    "    print(f\"\\n{'â”€'*80}\")\n",
    "    print(f\"ğŸ”§ LIMPIANDO: {nombre}\")\n",
    "    print(f\"{'â”€'*80}\")\n",
    "    \n",
    "    # Mostrar categorÃ­as originales\n",
    "    valores_antes = df[columna].value_counts()\n",
    "    categorias_antes = df[columna].nunique()\n",
    "    \n",
    "    print(f\"\\n CategorÃ­as originales en '{columna}':\")\n",
    "    print(f\"   Total categorÃ­as Ãºnicas: {categorias_antes}\")\n",
    "    print(f\"\\n   Listado completo:\")\n",
    "    for i, cat in enumerate(sorted(df[columna].unique()), 1):\n",
    "        count = (df[columna] == cat).sum()\n",
    "        pct = count / len(df) * 100\n",
    "        print(f\"      {i:2d}. {cat:<45} ({count:>5} registros, {pct:>5.2f}%)\")\n",
    "    \n",
    "    # Aplicar mapeo\n",
    "    print(f\"\\nâ³ Aplicando {len(mapeo_dict)} reglas de normalizaciÃ³n...\")\n",
    "    df[columna] = df[columna].replace(mapeo_dict)\n",
    "    \n",
    "    # Normalizar espacios\n",
    "    df[columna] = df[columna].str.strip()\n",
    "    \n",
    "    # Verificar resultados\n",
    "    valores_despues = df[columna].value_counts()\n",
    "    categorias_despues = df[columna].nunique()\n",
    "    categorias_eliminadas = categorias_antes - categorias_despues\n",
    "    \n",
    "    print(f\"\\n NormalizaciÃ³n completada:\")\n",
    "    print(f\"   â€¢ CategorÃ­as antes: {categorias_antes}\")\n",
    "    print(f\"   â€¢ CategorÃ­as despuÃ©s: {categorias_despues}\")\n",
    "    print(f\"   â€¢ CategorÃ­as eliminadas/unificadas: {categorias_eliminadas}\")\n",
    "    \n",
    "    # Mostrar categorÃ­as despuÃ©s de limpieza\n",
    "    print(f\"\\n CategorÃ­as normalizadas en '{columna}':\")\n",
    "    print(f\"   Total categorÃ­as Ãºnicas: {categorias_despues}\")\n",
    "    print(f\"\\n   Listado:\")\n",
    "    for i, cat in enumerate(sorted(df[columna].unique()), 1):\n",
    "        count = (df[columna] == cat).sum()\n",
    "        pct = count / len(df) * 100\n",
    "        print(f\"      {i:2d}. {cat:<45} ({count:>5} registros, {pct:>5.2f}%)\")\n",
    "    \n",
    "    # Mostrar cambios especÃ­ficos\n",
    "    if categorias_eliminadas > 0:\n",
    "        print(f\"\\n Detalle de cambios aplicados:\")\n",
    "        cambios_aplicados = 0\n",
    "        for orig, nuevo in mapeo_dict.items():\n",
    "            if orig in valores_antes.index:\n",
    "                count = valores_antes[orig]\n",
    "                cambios_aplicados += 1\n",
    "                print(f\"   âœ“ '{orig}' â†’ '{nuevo}' ({count} registros)\")\n",
    "        \n",
    "        if cambios_aplicados > 0:\n",
    "            print(f\"\\n    Total de transformaciones: {cambios_aplicados}\")\n",
    "            registros_afectados = sum(valores_antes[k] for k in mapeo_dict.keys() if k in valores_antes.index)\n",
    "            print(f\"    Registros afectados: {registros_afectados} ({registros_afectados/len(df)*100:.2f}%)\")\n",
    "    else:\n",
    "        print(f\"\\n    No se encontraron categorÃ­as para mapear (ya estaban limpias)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# MAPEO 1: PAÃS DE ORIGEN\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "mapeo_pais_origen = {\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # GRUPO: EEUU, CANADÃ Y MÃ‰XICO\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    'Ee.Uu Y CanadÃ¡': 'EEUU, CanadÃ¡ y MÃ©xico',\n",
    "    'Ee.Uu, CanadÃ¡': 'EEUU, CanadÃ¡ y MÃ©xico',\n",
    "    'Ee.Uu Y Canada': 'EEUU, CanadÃ¡ y MÃ©xico',\n",
    "    'Ee.Uu, Canada': 'EEUU, CanadÃ¡ y MÃ©xico',\n",
    "    'EEUU Y CanadÃ¡': 'EEUU, CanadÃ¡ y MÃ©xico',\n",
    "    'EEUU, CanadÃ¡': 'EEUU, CanadÃ¡ y MÃ©xico',\n",
    "    'Ee.Uu, CanadÃ¡ Y MÃ©xico': 'EEUU, CanadÃ¡ y MÃ©xico',\n",
    "    'Ee.Uu Y CanadÃ¡ Y MÃ©xico': 'EEUU, CanadÃ¡ y MÃ©xico',\n",
    "    'EEUU, Canada Y Mexico': 'EEUU, CanadÃ¡ y MÃ©xico',\n",
    "    'EEUU Y Canada': 'EEUU, CanadÃ¡ y MÃ©xico',\n",
    "    'EE.UU Y CanadÃ¡': 'EEUU, CanadÃ¡ y MÃ©xico',\n",
    "    'EE.UU, CanadÃ¡': 'EEUU, CanadÃ¡ y MÃ©xico',\n",
    "    'EE.UU, CanadÃ¡ Y MÃ©xico': 'EEUU, CanadÃ¡ y MÃ©xico',\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # GRUPO: EUROPA Y RESTO DEL MUNDO\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    'Europa': 'Europa y Resto del Mundo',\n",
    "    'Europa Y Resto Del Mundo': 'Europa y Resto del Mundo',\n",
    "    'Europa y Resto del Mundo': 'Europa y Resto del Mundo',\n",
    "    'Europa Y Resto Mundo': 'Europa y Resto del Mundo',\n",
    "    'EUROPA': 'Europa y Resto del Mundo',\n",
    "    'EUROPA Y RESTO DEL MUNDO': 'Europa y Resto del Mundo',\n",
    "    'Resto Del Mundo': 'Europa y Resto del Mundo',\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # GRUPO: RESTO DE AMÃ‰RICA\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    'Resto AmÃ©rica': 'Resto de AmÃ©rica',\n",
    "    'Resto America': 'Resto de AmÃ©rica',\n",
    "    'Resto De AmÃ©rica': 'Resto de AmÃ©rica',\n",
    "    'Resto De America': 'Resto de AmÃ©rica',\n",
    "    'RESTO AMÃ‰RICA': 'Resto de AmÃ©rica',\n",
    "    'RESTO AMERICA': 'Resto de AmÃ©rica',\n",
    "    'Resto AmÃ©rica ': 'Resto de AmÃ©rica',\n",
    "    ' Resto AmÃ©rica': 'Resto de AmÃ©rica',\n",
    "    'Resto  AmÃ©rica': 'Resto de AmÃ©rica',\n",
    "}\n",
    "\n",
    "# Aplicar limpieza a pais_origen\n",
    "if 'pais_origen' in df.columns:\n",
    "    df = limpiar_columna_categorica(\n",
    "        df, \n",
    "        'pais_origen', \n",
    "        mapeo_pais_origen,\n",
    "        \"PAÃS DE ORIGEN\"\n",
    "    )\n",
    "else:\n",
    "    print(\" Columna 'pais_origen' no encontrada\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# MAPEO 2: PUNTO DE ENTRADA\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "mapeo_punto_entrada = {\n",
    "    'Aeropuerto Bs As': 'Aeropuerto Buenos Aires',\n",
    "    'Aeropuerto BA': 'Aeropuerto Buenos Aires',\n",
    "    'Aerop. Buenos Aires': 'Aeropuerto Buenos Aires',\n",
    "    'Paso Cristo Redentor ': 'Paso Cristo Redentor',\n",
    "    ' Paso Cristo Redentor': 'Paso Cristo Redentor',\n",
    "}\n",
    "\n",
    "# Aplicar limpieza a punto_entrada\n",
    "if 'punto_entrada' in df.columns:\n",
    "    df = limpiar_columna_categorica(\n",
    "        df, \n",
    "        'punto_entrada', \n",
    "        mapeo_punto_entrada,\n",
    "        \"PUNTO DE ENTRADA\"\n",
    "    )\n",
    "else:\n",
    "    print(\" Columna 'punto_entrada' no encontrada\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# RESUMEN FINAL\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" LIMPIEZA DE CATEGORÃAS COMPLETADA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n Resumen final del dataset:\")\n",
    "print(f\"   â€¢ Total registros: {len(df):,}\")\n",
    "\n",
    "if 'pais_origen' in df.columns:\n",
    "    print(f\"\\n    PaÃ­s de Origen:\")\n",
    "    print(f\"      â€¢ CategorÃ­as Ãºnicas: {df['pais_origen'].nunique()}\")\n",
    "    print(f\"      â€¢ Sin valores nulos: {df['pais_origen'].notna().sum()}\")\n",
    "    print(f\"\\n      Top 5:\")\n",
    "    for i, (cat, count) in enumerate(df['pais_origen'].value_counts().head(5).items(), 1):\n",
    "        pct = count / len(df) * 100\n",
    "        print(f\"         {i}. {cat:<40} ({count:>5}, {pct:>5.2f}%)\")\n",
    "\n",
    "if 'punto_entrada' in df.columns:\n",
    "    print(f\"\\n    Punto de Entrada:\")\n",
    "    print(f\"      â€¢ CategorÃ­as Ãºnicas: {df['punto_entrada'].nunique()}\")\n",
    "    print(f\"      â€¢ Sin valores nulos: {df['punto_entrada'].notna().sum()}\")\n",
    "    print(f\"\\n      Top 5:\")\n",
    "    for i, (cat, count) in enumerate(df['punto_entrada'].value_counts().head(5).items(), 1):\n",
    "        pct = count / len(df) * 100\n",
    "        print(f\"         {i}. {cat:<40} ({count:>5}, {pct:>5.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" Dataset listo para anÃ¡lisis con categorÃ­as normalizadas\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e42768e1-a13a-4989-800a-d3427e2c2407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ§¹ LIMPIEZA DE DATOS\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Dataset antes de limpieza:\n",
      "   â€¢ Total de registros: 2,877\n",
      "   â€¢ Registros con 0 turistas: 389\n",
      "   â€¢ Porcentaje de ceros: 13.52%\n",
      "\n",
      "ğŸ” AnÃ¡lisis de registros a eliminar:\n",
      "\n",
      "   ğŸ“… DistribuciÃ³n temporal de los ceros:\n",
      "      â€¢ 2020: 207 registros (75.0% del aÃ±o)\n",
      "      â€¢ 2021: 182 registros (65.9% del aÃ±o)\n",
      "\n",
      "   ğŸ“Š DistribuciÃ³n por pais_origen (top 5):\n",
      "      â€¢ Europa y Resto del Mundo: 102 registros\n",
      "      â€¢ Chile: 69 registros\n",
      "      â€¢ EEUU, CanadÃ¡ y MÃ©xico: 51 registros\n",
      "      â€¢ Brasil: 51 registros\n",
      "      â€¢ Resto de AmÃ©rica: 51 registros\n",
      "\n",
      "ğŸ’¡ DECISIÃ“N: Eliminar todos los registros con 0 turistas\n",
      "\n",
      "   JustificaciÃ³n:\n",
      "   âœ“ Representan situaciones excepcionales (cierres, pandemias)\n",
      "   âœ“ NO reflejan la demanda turÃ­stica normal operativa\n",
      "   âœ“ DistorsionarÃ­an el entrenamiento y las mÃ©tricas\n",
      "   âœ“ El objetivo es predecir demanda en condiciones normales\n",
      "   âœ“ Mejora la utilidad prÃ¡ctica del modelo\n",
      "\n",
      "âœ… Limpieza ejecutada:\n",
      "   â€¢ Registros eliminados: 389\n",
      "   â€¢ Registros restantes: 2,488 (86.5%)\n",
      "\n",
      "ğŸ“Š Dataset despuÃ©s de limpieza:\n",
      "   â€¢ Nueva media: 11,399 turistas\n",
      "   â€¢ Nueva mediana: 3,627 turistas\n",
      "   â€¢ Nueva desv. estÃ¡ndar: 15,654\n",
      "   â€¢ Nuevo rango: 12 - 88,814\n",
      "\n",
      "ğŸ“Š Impacto de la limpieza:\n",
      "   â€¢ Cambio en media: +1,541 turistas\n",
      "   â€¢ Cambio en std: +584\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š PREPARACIÃ“N DE VARIABLES\n",
      "================================================================================\n",
      "âœ… Features (X): (2488, 27)\n",
      "âœ… Target (y): (2488,)\n",
      "\n",
      "   Variables predictoras:\n",
      "      â€¢ NumÃ©ricas: 25\n",
      "      â€¢ CategÃ³ricas: 2\n",
      "\n",
      "ğŸ” VerificaciÃ³n de integridad:\n",
      "   â€¢ Valores nulos en X: 0\n",
      "   â€¢ Valores nulos en y: 0\n",
      "   â€¢ Valores cero en y: 0\n",
      "   â€¢ MÃ­nimo en y: 12\n",
      "   â€¢ MÃ¡ximo en y: 88,814\n",
      "   âœ… Sin ceros en variable objetivo - Dataset limpio\n",
      "\n",
      "================================================================================\n",
      "âœ‚ï¸ ESTRATEGIA DE PARTICIÃ“N\n",
      "================================================================================\n",
      "\n",
      "Estrategia elegida: SPLIT TEMPORAL (80/20)\n",
      "\n",
      "JustificaciÃ³n:\n",
      "   â€¢ Los datos son series temporales (mensuales)\n",
      "   â€¢ Queremos simular predicciÃ³n del futuro con datos del pasado\n",
      "   â€¢ Train/Test respeta el orden cronolÃ³gico\n",
      "   â€¢ Evita data leakage temporal\n",
      "\n",
      "Â¿Por quÃ© temporal y no aleatorio?\n",
      "   âœ“ En datos temporales, el orden importa\n",
      "   âœ“ Split aleatorio mezclarÃ­a pasado y futuro (data leakage)\n",
      "   âœ“ Queremos evaluar cÃ³mo predice el modelo en perÃ­odos futuros\n",
      "   âœ“ Es mÃ¡s realista para poner el modelo en producciÃ³n\n",
      "\n",
      "DistribuciÃ³n:\n",
      "   â€¢ Train: 80% primeros datos (aprende de historia)\n",
      "   â€¢ Test: 20% Ãºltimos datos (evalÃºa en futuro)\n",
      "\n",
      "âœ… Split temporal ejecutado:\n",
      "   â€¢ Train set: 1,990 registros (80.0%)\n",
      "   â€¢ Test set: 498 registros (20.0%)\n",
      "\n",
      "ğŸ“Š EstadÃ­sticas de la variable objetivo por conjunto:\n",
      "\n",
      "   TRAIN:\n",
      "      â€¢ Media: 11,359 Â± 15,489\n",
      "      â€¢ Mediana: 3,638\n",
      "      â€¢ Rango: 12 - 88,814\n",
      "      â€¢ Ceros: 0\n",
      "\n",
      "   TEST:\n",
      "      â€¢ Media: 11,559 Â± 16,314\n",
      "      â€¢ Mediana: 3,534\n",
      "      â€¢ Rango: 344 - 87,279\n",
      "      â€¢ Ceros: 0\n",
      "\n",
      "   ğŸ“Š ComparaciÃ³n:\n",
      "      â€¢ Diferencia en media: +1.76%\n",
      "      âœ… Distribuciones similares entre train y test\n",
      "\n",
      "   âœ… CONFIRMACIÃ“N: Sin ceros en train ni test\n",
      "      â†’ La limpieza de datos fue exitosa\n",
      "\n",
      "================================================================================\n",
      "ğŸ”§ CONSTRUCCIÃ“N DEL PIPELINE DE SCIKIT-LEARN\n",
      "================================================================================\n",
      "\n",
      "Pipeline de preprocesamiento con ColumnTransformer:\n",
      "\n",
      "Â¿QuÃ© es un pipeline?\n",
      "   â€¢ Secuencia de transformaciones que se aplican automÃ¡ticamente\n",
      "   â€¢ Garantiza que train y test se procesan EXACTAMENTE igual\n",
      "   â€¢ Evita data leakage (los parÃ¡metros se aprenden solo del train)\n",
      "   â€¢ Facilita reproducibilidad y deployment\n",
      "\n",
      "Estructura del pipeline:\n",
      "\n",
      "1ï¸âƒ£ VARIABLES NUMÃ‰RICAS:\n",
      "   a) ImputaciÃ³n: SimpleImputer (strategy='median')\n",
      "      â†’ Rellena valores faltantes con la mediana del train\n",
      "      â†’ Â¿Por quÃ© mediana? Es robusta a outliers\n",
      "      â†’ Se calcula SOLO en train, se aplica en train y test\n",
      "\n",
      "   b) Escalado: StandardScaler\n",
      "      â†’ Normaliza a media=0, desviaciÃ³n estÃ¡ndar=1\n",
      "      â†’ Â¿Por quÃ©? Modelos como Ridge son sensibles a escala\n",
      "      â†’ FÃ³rmula: (x - media) / std\n",
      "      â†’ Se calcula SOLO en train, se aplica en train y test\n",
      "\n",
      "2ï¸âƒ£ VARIABLES CATEGÃ“RICAS:\n",
      "   a) ImputaciÃ³n: SimpleImputer (strategy='constant', fill_value='missing')\n",
      "      â†’ Rellena valores nulos con la categorÃ­a 'missing'\n",
      "      â†’ Crea una categorÃ­a explÃ­cita para datos faltantes\n",
      "\n",
      "   b) Encoding: OneHotEncoder (handle_unknown='ignore')\n",
      "      â†’ Transforma categorÃ­as en columnas binarias (0/1)\n",
      "      â†’ Ejemplo: paÃ­s=['Chile', 'Brasil'] â†’ [paÃ­s_Chile, paÃ­s_Brasil]\n",
      "      â†’ handle_unknown='ignore': si test tiene categorÃ­a nueva, la ignora\n",
      "\n",
      "3ï¸âƒ£ INTEGRACIÃ“N CON COLUMNTRANSFORMER:\n",
      "   â€¢ Aplica transformaciones en paralelo por tipo de variable\n",
      "   â€¢ Concatena resultados en un solo array\n",
      "   â€¢ Compatible con cualquier modelo de Scikit-learn\n",
      "   â€¢ Usa todos los cores disponibles (n_jobs=-1)\n",
      "\n",
      "âœ… Transformador numÃ©rico creado:\n",
      "   1. SimpleImputer(strategy='median')\n",
      "   2. StandardScaler()\n",
      "\n",
      "âœ… Transformador categÃ³rico creado:\n",
      "   1. SimpleImputer(strategy='constant', fill_value='missing')\n",
      "   2. OneHotEncoder(handle_unknown='ignore')\n",
      "\n",
      "âœ… ColumnTransformer creado:\n",
      "   â€¢ Transformador numÃ©rico: 25 variables\n",
      "   â€¢ Transformador categÃ³rico: 2 variables\n",
      "   â€¢ ParalelizaciÃ³n: n_jobs=-1 (todos los cores)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "ğŸ§ª AJUSTE Y VERIFICACIÃ“N DEL PIPELINE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "â³ Ajustando preprocessor en datos de train...\n",
      "   (Aprendiendo parÃ¡metros: mediana, media, std, categorÃ­as...)\n",
      "âœ… Preprocessor ajustado exitosamente\n",
      "\n",
      "â³ Transformando datos...\n",
      "\n",
      "âœ… TransformaciÃ³n completada:\n",
      "\n",
      "   Train:\n",
      "      â€¢ Shape original: (1990, 27)\n",
      "      â€¢ Shape transformado: (1990, 38)\n",
      "      â€¢ Tipo: <class 'numpy.ndarray'>\n",
      "\n",
      "   Test:\n",
      "      â€¢ Shape original: (498, 27)\n",
      "      â€¢ Shape transformado: (498, 38)\n",
      "      â€¢ Tipo: <class 'numpy.ndarray'>\n",
      "\n",
      "ğŸ“Š AnÃ¡lisis de dimensionalidad:\n",
      "   â€¢ Features numÃ©ricas: 25\n",
      "   â€¢ Features categÃ³ricas originales: 2\n",
      "   â€¢ Columnas creadas por one-hot: 13\n",
      "   â€¢ Total features transformadas: 38\n",
      "   âœ… Dimensionalidad baja (<50) - Ã³ptima para modelado\n",
      "\n",
      "ğŸ” VerificaciÃ³n de calidad:\n",
      "   â€¢ NaN en train transformado: 0\n",
      "   â€¢ NaN en test transformado: 0\n",
      "   â€¢ Inf en train transformado: 0\n",
      "   â€¢ Inf en test transformado: 0\n",
      "   âœ… Datos transformados limpios (sin NaN ni infinitos)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "ğŸ“Š ESTRUCTURA DEL PIPELINE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ğŸ”§ Diagrama del preprocesador:\n",
      "ColumnTransformer(n_jobs=-1,\n",
      "                  transformers=[('num',\n",
      "                                 Pipeline(steps=[('imputer',\n",
      "                                                  SimpleImputer(strategy='median')),\n",
      "                                                 ('scaler', StandardScaler())]),\n",
      "                                 ['precio_promedio_usd', 'precio_minimo_usd',\n",
      "                                  'precio_maximo_usd', 'dias',\n",
      "                                  'variacion_usd_mensual',\n",
      "                                  'variacion_porcentual_usd', 'usd_alto',\n",
      "                                  'usd_alta_variabilidad', 'aÃ±o', 'mes',\n",
      "                                  'es_enero', 'es_febrero', 'es_marzo',\n",
      "                                  'es_abr...o', 'es_junio', 'es_julio',\n",
      "                                  'es_agosto', 'es_septiembre', 'es_octubre',\n",
      "                                  'es_noviembre', 'es_diciembre',\n",
      "                                  'mes_vendimia', 'vacaciones_invierno',\n",
      "                                  'temporada_alta']),\n",
      "                                ('cat',\n",
      "                                 Pipeline(steps=[('imputer',\n",
      "                                                  SimpleImputer(fill_value='missing',\n",
      "                                                                strategy='constant')),\n",
      "                                                 ('encoder',\n",
      "                                                  OneHotEncoder(handle_unknown='ignore',\n",
      "                                                                sparse_output=False))]),\n",
      "                                 ['pais_origen', 'punto_entrada'])])\n",
      "\n",
      "ğŸ“‹ Resumen del pipeline:\n",
      "\n",
      "ColumnTransformer(\n",
      "   transformers=[\n",
      "      ('num', Pipeline([\n",
      "         SimpleImputer(strategy='median'),\n",
      "         StandardScaler()\n",
      "      ]), 25 variables numÃ©ricas),\n",
      "\n",
      "      ('cat', Pipeline([\n",
      "         SimpleImputer(strategy='constant'),\n",
      "         OneHotEncoder(handle_unknown='ignore')\n",
      "      ]), 2 variables categÃ³ricas)\n",
      "   ],\n",
      "   remainder='drop',\n",
      "   n_jobs=-1\n",
      ")\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ğŸ“‹ RESUMEN DE LA PREPARACIÃ“N\n",
      "================================================================================\n",
      "\n",
      "âœ… LIMPIEZA COMPLETADA:\n",
      "   â€¢ Registros eliminados: 389 (0 turistas)\n",
      "   â€¢ Registros finales: 2,488\n",
      "   â€¢ Sin ceros en variable objetivo\n",
      "\n",
      "âœ… SPLIT TEMPORAL EJECUTADO:\n",
      "   â€¢ Train: 1,990 (80.0%)\n",
      "   â€¢ Test: 498 (20.0%)\n",
      "   â€¢ Sin ceros en train ni test\n",
      "\n",
      "âœ… PIPELINE CONSTRUIDO Y AJUSTADO:\n",
      "   â€¢ Preprocessor fitted en train\n",
      "   â€¢ Transformaciones aplicadas en train y test\n",
      "   â€¢ Dimensionalidad: 27 â†’ 38\n",
      "   â€¢ Sin NaN ni infinitos\n",
      "\n",
      "âœ… DATOS LISTOS PARA MODELADO:\n",
      "   â€¢ X_train: (1990, 27)\n",
      "   â€¢ X_test: (498, 27)\n",
      "   â€¢ y_train: (1990,)\n",
      "   â€¢ y_test: (498,)\n",
      "   â€¢ preprocessor: Listo para integrar con modelos\n",
      "\n",
      "ğŸ¯ PRÃ“XIMOS PASOS:\n",
      "   1. Definir modelos (Ridge, Random Forest, Gradient Boosting)\n",
      "   2. Crear pipelines completos (preprocessor + modelo)\n",
      "   3. Entrenar con validaciÃ³n cruzada\n",
      "   4. Evaluar y comparar modelos\n",
      "   5. Seleccionar el mejor\n",
      "\n",
      "================================================================================\n",
      "âœ… PREPARACIÃ“N COMPLETADA - PIPELINE LISTO PARA MODELADO\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\"\"\"\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "PASO 2: PREPARACIÃ“N DE DATOS Y CONSTRUCCIÃ“N DEL PIPELINE\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "Objetivo: Crear un pipeline completo de Scikit-learn que integre:\n",
    "   1. Limpieza de datos (eliminaciÃ³n de registros con 0 turistas)\n",
    "   2. Preprocesamiento de variables numÃ©ricas (imputaciÃ³n + escalado)\n",
    "   3. Preprocesamiento de variables categÃ³ricas (imputaciÃ³n + encoding)\n",
    "   4. ParticiÃ³n train/test temporal\n",
    "   \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# LIMPIEZA DE DATOS: ELIMINACIÃ“N DE REGISTROS CON 0 TURISTAS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ§¹ LIMPIEZA DE DATOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nğŸ“Š Dataset antes de limpieza:\")\n",
    "print(f\"   â€¢ Total de registros: {len(df):,}\")\n",
    "print(f\"   â€¢ Registros con 0 turistas: {(df['turistas'] == 0).sum():,}\")\n",
    "print(f\"   â€¢ Porcentaje de ceros: {(df['turistas'] == 0).sum() / len(df) * 100:.2f}%\")\n",
    "\n",
    "# AnÃ¡lisis previo a eliminaciÃ³n\n",
    "ceros_count_before = (df['turistas'] == 0).sum()\n",
    "\n",
    "if ceros_count_before > 0:\n",
    "    print(f\"\\nğŸ” AnÃ¡lisis de registros a eliminar:\")\n",
    "    \n",
    "    # EstadÃ­sticas de los ceros\n",
    "    df_ceros = df[df['turistas'] == 0]\n",
    "    \n",
    "    if 'aÃ±o' in df.columns:\n",
    "        print(f\"\\n   ğŸ“… DistribuciÃ³n temporal de los ceros:\")\n",
    "        ceros_por_aÃ±o = df_ceros.groupby('aÃ±o').size()\n",
    "        for aÃ±o, count in ceros_por_aÃ±o.items():\n",
    "            total_aÃ±o = (df['aÃ±o'] == aÃ±o).sum()\n",
    "            pct = count / total_aÃ±o * 100\n",
    "            print(f\"      â€¢ {aÃ±o}: {count} registros ({pct:.1f}% del aÃ±o)\")\n",
    "    \n",
    "    if categorical_features:\n",
    "        cat_principal = categorical_features[0]\n",
    "        print(f\"\\n   ğŸ“Š DistribuciÃ³n por {cat_principal} (top 5):\")\n",
    "        ceros_por_cat = df_ceros.groupby(cat_principal).size().sort_values(ascending=False).head(5)\n",
    "        for cat, count in ceros_por_cat.items():\n",
    "            print(f\"      â€¢ {cat}: {count} registros\")\n",
    "    \n",
    "    print(f\"\\nğŸ’¡ DECISIÃ“N: Eliminar todos los registros con 0 turistas\")\n",
    "    print(f\"\\n   JustificaciÃ³n:\")\n",
    "    print(f\"   âœ“ Representan situaciones excepcionales (cierres, pandemias)\")\n",
    "    print(f\"   âœ“ NO reflejan la demanda turÃ­stica normal operativa\")\n",
    "    print(f\"   âœ“ DistorsionarÃ­an el entrenamiento y las mÃ©tricas\")\n",
    "    print(f\"   âœ“ El objetivo es predecir demanda en condiciones normales\")\n",
    "    print(f\"   âœ“ Mejora la utilidad prÃ¡ctica del modelo\")\n",
    "    \n",
    "    # Eliminar registros con 0 turistas\n",
    "    df_clean = df[df['turistas'] > 0].copy()\n",
    "    \n",
    "    print(f\"\\nâœ… Limpieza ejecutada:\")\n",
    "    print(f\"   â€¢ Registros eliminados: {len(df) - len(df_clean):,}\")\n",
    "    print(f\"   â€¢ Registros restantes: {len(df_clean):,} ({len(df_clean)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Actualizar dataset\n",
    "    df = df_clean\n",
    "    \n",
    "    # EstadÃ­sticas despuÃ©s de limpieza\n",
    "    print(f\"\\nğŸ“Š Dataset despuÃ©s de limpieza:\")\n",
    "    print(f\"   â€¢ Nueva media: {df['turistas'].mean():,.0f} turistas\")\n",
    "    print(f\"   â€¢ Nueva mediana: {df['turistas'].median():,.0f} turistas\")\n",
    "    print(f\"   â€¢ Nueva desv. estÃ¡ndar: {df['turistas'].std():,.0f}\")\n",
    "    print(f\"   â€¢ Nuevo rango: {df['turistas'].min():,.0f} - {df['turistas'].max():,.0f}\")\n",
    "    \n",
    "    # ComparaciÃ³n antes/despuÃ©s\n",
    "    print(f\"\\nğŸ“Š Impacto de la limpieza:\")\n",
    "    print(f\"   â€¢ Cambio en media: {df['turistas'].mean() - df_original['turistas'].mean():+,.0f} turistas\")\n",
    "    print(f\"   â€¢ Cambio en std: {df['turistas'].std() - df_original['turistas'].std():+,.0f}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"   âœ… No hay registros con 0 turistas para eliminar\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SEPARACIÃ“N FEATURES Y TARGET\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š PREPARACIÃ“N DE VARIABLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "X = df[numeric_features + categorical_features].copy()\n",
    "y = df['turistas'].copy()\n",
    "\n",
    "print(f\"âœ… Features (X): {X.shape}\")\n",
    "print(f\"âœ… Target (y): {y.shape}\")\n",
    "print(f\"\\n   Variables predictoras:\")\n",
    "print(f\"      â€¢ NumÃ©ricas: {len(numeric_features)}\")\n",
    "print(f\"      â€¢ CategÃ³ricas: {len(categorical_features)}\")\n",
    "\n",
    "# VerificaciÃ³n de integridad\n",
    "print(f\"\\nğŸ” VerificaciÃ³n de integridad:\")\n",
    "print(f\"   â€¢ Valores nulos en X: {X.isnull().sum().sum()}\")\n",
    "print(f\"   â€¢ Valores nulos en y: {y.isnull().sum()}\")\n",
    "print(f\"   â€¢ Valores cero en y: {(y == 0).sum()}\")\n",
    "print(f\"   â€¢ MÃ­nimo en y: {y.min():,.0f}\")\n",
    "print(f\"   â€¢ MÃ¡ximo en y: {y.max():,.0f}\")\n",
    "\n",
    "if (y == 0).sum() == 0:\n",
    "    print(f\"   âœ… Sin ceros en variable objetivo - Dataset limpio\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ESTRATEGIA DE PARTICIÃ“N\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ‚ï¸ ESTRATEGIA DE PARTICIÃ“N\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "Estrategia elegida: SPLIT TEMPORAL (80/20)\n",
    "\n",
    "JustificaciÃ³n:\n",
    "   â€¢ Los datos son series temporales (mensuales)\n",
    "   â€¢ Queremos simular predicciÃ³n del futuro con datos del pasado\n",
    "   â€¢ Train/Test respeta el orden cronolÃ³gico\n",
    "   â€¢ Evita data leakage temporal\n",
    "\n",
    "Â¿Por quÃ© temporal y no aleatorio?\n",
    "   âœ“ En datos temporales, el orden importa\n",
    "   âœ“ Split aleatorio mezclarÃ­a pasado y futuro (data leakage)\n",
    "   âœ“ Queremos evaluar cÃ³mo predice el modelo en perÃ­odos futuros\n",
    "   âœ“ Es mÃ¡s realista para poner el modelo en producciÃ³n\n",
    "\n",
    "DistribuciÃ³n:\n",
    "   â€¢ Train: 80% primeros datos (aprende de historia)\n",
    "   â€¢ Test: 20% Ãºltimos datos (evalÃºa en futuro)\n",
    "\"\"\")\n",
    "\n",
    "# Split temporal\n",
    "split_index = int(len(X) * 0.8)\n",
    "X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]\n",
    "y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]\n",
    "\n",
    "print(f\"âœ… Split temporal ejecutado:\")\n",
    "print(f\"   â€¢ Train set: {len(X_train):,} registros ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"   â€¢ Test set: {len(X_test):,} registros ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "# EstadÃ­sticas train/test\n",
    "print(f\"\\nğŸ“Š EstadÃ­sticas de la variable objetivo por conjunto:\")\n",
    "print(f\"\\n   TRAIN:\")\n",
    "print(f\"      â€¢ Media: {y_train.mean():,.0f} Â± {y_train.std():,.0f}\")\n",
    "print(f\"      â€¢ Mediana: {y_train.median():,.0f}\")\n",
    "print(f\"      â€¢ Rango: {y_train.min():,.0f} - {y_train.max():,.0f}\")\n",
    "print(f\"      â€¢ Ceros: {(y_train == 0).sum()}\")\n",
    "\n",
    "print(f\"\\n   TEST:\")\n",
    "print(f\"      â€¢ Media: {y_test.mean():,.0f} Â± {y_test.std():,.0f}\")\n",
    "print(f\"      â€¢ Mediana: {y_test.median():,.0f}\")\n",
    "print(f\"      â€¢ Rango: {y_test.min():,.0f} - {y_test.max():,.0f}\")\n",
    "print(f\"      â€¢ Ceros: {(y_test == 0).sum()}\")\n",
    "\n",
    "# AnÃ¡lisis de diferencia train/test\n",
    "diff_mean = ((y_test.mean() - y_train.mean()) / y_train.mean()) * 100\n",
    "print(f\"\\n   ğŸ“Š ComparaciÃ³n:\")\n",
    "print(f\"      â€¢ Diferencia en media: {diff_mean:+.2f}%\")\n",
    "\n",
    "if abs(diff_mean) > 20:\n",
    "    print(f\"      âš ï¸ Gran diferencia entre train y test\")\n",
    "    print(f\"         â†’ Esto es NORMAL en split temporal (tendencias temporales)\")\n",
    "    print(f\"         â†’ El modelo aprenderÃ¡ de un perÃ­odo y predecirÃ¡ otro\")\n",
    "elif abs(diff_mean) > 10:\n",
    "    print(f\"      ğŸ“Š Diferencia moderada entre train y test\")\n",
    "    print(f\"         â†’ Esperado en datos temporales con tendencias\")\n",
    "else:\n",
    "    print(f\"      âœ… Distribuciones similares entre train y test\")\n",
    "\n",
    "# Verificar que no hay ceros\n",
    "if (y_train == 0).sum() == 0 and (y_test == 0).sum() == 0:\n",
    "    print(f\"\\n   âœ… CONFIRMACIÃ“N: Sin ceros en train ni test\")\n",
    "    print(f\"      â†’ La limpieza de datos fue exitosa\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CONSTRUCCIÃ“N DEL PIPELINE DE PREPROCESAMIENTO\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ”§ CONSTRUCCIÃ“N DEL PIPELINE DE SCIKIT-LEARN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "Pipeline de preprocesamiento con ColumnTransformer:\n",
    "\n",
    "Â¿QuÃ© es un pipeline?\n",
    "   â€¢ Secuencia de transformaciones que se aplican automÃ¡ticamente\n",
    "   â€¢ Garantiza que train y test se procesan EXACTAMENTE igual\n",
    "   â€¢ Evita data leakage (los parÃ¡metros se aprenden solo del train)\n",
    "   â€¢ Facilita reproducibilidad y deployment\n",
    "\n",
    "Estructura del pipeline:\n",
    "\n",
    "1ï¸âƒ£ VARIABLES NUMÃ‰RICAS:\n",
    "   a) ImputaciÃ³n: SimpleImputer (strategy='median')\n",
    "      â†’ Rellena valores faltantes con la mediana del train\n",
    "      â†’ Â¿Por quÃ© mediana? Es robusta a outliers\n",
    "      â†’ Se calcula SOLO en train, se aplica en train y test\n",
    "   \n",
    "   b) Escalado: StandardScaler\n",
    "      â†’ Normaliza a media=0, desviaciÃ³n estÃ¡ndar=1\n",
    "      â†’ Â¿Por quÃ©? Modelos como Ridge son sensibles a escala\n",
    "      â†’ FÃ³rmula: (x - media) / std\n",
    "      â†’ Se calcula SOLO en train, se aplica en train y test\n",
    "\n",
    "2ï¸âƒ£ VARIABLES CATEGÃ“RICAS:\n",
    "   a) ImputaciÃ³n: SimpleImputer (strategy='constant', fill_value='missing')\n",
    "      â†’ Rellena valores nulos con la categorÃ­a 'missing'\n",
    "      â†’ Crea una categorÃ­a explÃ­cita para datos faltantes\n",
    "   \n",
    "   b) Encoding: OneHotEncoder (handle_unknown='ignore')\n",
    "      â†’ Transforma categorÃ­as en columnas binarias (0/1)\n",
    "      â†’ Ejemplo: paÃ­s=['Chile', 'Brasil'] â†’ [paÃ­s_Chile, paÃ­s_Brasil]\n",
    "      â†’ handle_unknown='ignore': si test tiene categorÃ­a nueva, la ignora\n",
    "\n",
    "3ï¸âƒ£ INTEGRACIÃ“N CON COLUMNTRANSFORMER:\n",
    "   â€¢ Aplica transformaciones en paralelo por tipo de variable\n",
    "   â€¢ Concatena resultados en un solo array\n",
    "   â€¢ Compatible con cualquier modelo de Scikit-learn\n",
    "   â€¢ Usa todos los cores disponibles (n_jobs=-1)\n",
    "\"\"\")\n",
    "\n",
    "# Transformador para variables numÃ©ricas\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "print(\"âœ… Transformador numÃ©rico creado:\")\n",
    "print(\"   1. SimpleImputer(strategy='median')\")\n",
    "print(\"   2. StandardScaler()\")\n",
    "\n",
    "# Transformador para variables categÃ³ricas\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "print(\"\\nâœ… Transformador categÃ³rico creado:\")\n",
    "print(\"   1. SimpleImputer(strategy='constant', fill_value='missing')\")\n",
    "print(\"   2. OneHotEncoder(handle_unknown='ignore')\")\n",
    "\n",
    "# ColumnTransformer combina ambos\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop',  # Descartar columnas no especificadas\n",
    "    n_jobs=-1  # Paralelizar transformaciones\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… ColumnTransformer creado:\")\n",
    "print(f\"   â€¢ Transformador numÃ©rico: {len(numeric_features)} variables\")\n",
    "print(f\"   â€¢ Transformador categÃ³rico: {len(categorical_features)} variables\")\n",
    "print(f\"   â€¢ ParalelizaciÃ³n: n_jobs=-1 (todos los cores)\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# AJUSTE Y VERIFICACIÃ“N DEL PIPELINE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"ğŸ§ª AJUSTE Y VERIFICACIÃ“N DEL PIPELINE\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(f\"\\nâ³ Ajustando preprocessor en datos de train...\")\n",
    "print(f\"   (Aprendiendo parÃ¡metros: mediana, media, std, categorÃ­as...)\")\n",
    "\n",
    "# Ajustar preprocessor (aprende parÃ¡metros del train)\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "print(f\"âœ… Preprocessor ajustado exitosamente\")\n",
    "\n",
    "# Transformar ambos conjuntos\n",
    "print(f\"\\nâ³ Transformando datos...\")\n",
    "X_train_transformed = preprocessor.transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"\\nâœ… TransformaciÃ³n completada:\")\n",
    "print(f\"\\n   Train:\")\n",
    "print(f\"      â€¢ Shape original: {X_train.shape}\")\n",
    "print(f\"      â€¢ Shape transformado: {X_train_transformed.shape}\")\n",
    "print(f\"      â€¢ Tipo: {type(X_train_transformed)}\")\n",
    "\n",
    "print(f\"\\n   Test:\")\n",
    "print(f\"      â€¢ Shape original: {X_test.shape}\")\n",
    "print(f\"      â€¢ Shape transformado: {X_test_transformed.shape}\")\n",
    "print(f\"      â€¢ Tipo: {type(X_test_transformed)}\")\n",
    "\n",
    "# Calcular dimensionalidad\n",
    "n_features_out = X_train_transformed.shape[1]\n",
    "n_features_cat_created = n_features_out - len(numeric_features)\n",
    "\n",
    "print(f\"\\nğŸ“Š AnÃ¡lisis de dimensionalidad:\")\n",
    "print(f\"   â€¢ Features numÃ©ricas: {len(numeric_features)}\")\n",
    "print(f\"   â€¢ Features categÃ³ricas originales: {len(categorical_features)}\")\n",
    "print(f\"   â€¢ Columnas creadas por one-hot: {n_features_cat_created}\")\n",
    "print(f\"   â€¢ Total features transformadas: {n_features_out}\")\n",
    "\n",
    "if n_features_out > 100:\n",
    "    print(f\"   âš ï¸ Dimensionalidad alta (>{100}) - puede ralentizar entrenamiento\")\n",
    "elif n_features_out > 50:\n",
    "    print(f\"   ğŸ“Š Dimensionalidad moderada (50-100)\")\n",
    "else:\n",
    "    print(f\"   âœ… Dimensionalidad baja (<50) - Ã³ptima para modelado\")\n",
    "\n",
    "# Verificar que no hay NaN o infinitos\n",
    "print(f\"\\nğŸ” VerificaciÃ³n de calidad:\")\n",
    "print(f\"   â€¢ NaN en train transformado: {np.isnan(X_train_transformed).sum()}\")\n",
    "print(f\"   â€¢ NaN en test transformado: {np.isnan(X_test_transformed).sum()}\")\n",
    "print(f\"   â€¢ Inf en train transformado: {np.isinf(X_train_transformed).sum()}\")\n",
    "print(f\"   â€¢ Inf en test transformado: {np.isinf(X_test_transformed).sum()}\")\n",
    "\n",
    "if (np.isnan(X_train_transformed).sum() == 0 and \n",
    "    np.isnan(X_test_transformed).sum() == 0 and\n",
    "    np.isinf(X_train_transformed).sum() == 0 and\n",
    "    np.isinf(X_test_transformed).sum() == 0):\n",
    "    print(f\"   âœ… Datos transformados limpios (sin NaN ni infinitos)\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# VISUALIZACIÃ“N DE LA ESTRUCTURA DEL PIPELINE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"ğŸ“Š ESTRUCTURA DEL PIPELINE\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "\n",
    "print(\"\\nğŸ”§ Diagrama del preprocesador:\")\n",
    "print(preprocessor)\n",
    "\n",
    "# Mostrar resumen textual\n",
    "print(\"\\nğŸ“‹ Resumen del pipeline:\")\n",
    "print(f\"\"\"\n",
    "ColumnTransformer(\n",
    "   transformers=[\n",
    "      ('num', Pipeline([\n",
    "         SimpleImputer(strategy='median'),\n",
    "         StandardScaler()\n",
    "      ]), {len(numeric_features)} variables numÃ©ricas),\n",
    "      \n",
    "      ('cat', Pipeline([\n",
    "         SimpleImputer(strategy='constant'),\n",
    "         OneHotEncoder(handle_unknown='ignore')\n",
    "      ]), {len(categorical_features)} variables categÃ³ricas)\n",
    "   ],\n",
    "   remainder='drop',\n",
    "   n_jobs=-1\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# RESUMEN DE LA PREPARACIÃ“N\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“‹ RESUMEN DE LA PREPARACIÃ“N\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "âœ… LIMPIEZA COMPLETADA:\n",
    "   â€¢ Registros eliminados: {ceros_count_before} (0 turistas)\n",
    "   â€¢ Registros finales: {len(df):,}\n",
    "   â€¢ Sin ceros en variable objetivo\n",
    "\n",
    "âœ… SPLIT TEMPORAL EJECUTADO:\n",
    "   â€¢ Train: {len(X_train):,} ({len(X_train)/len(X)*100:.1f}%)\n",
    "   â€¢ Test: {len(X_test):,} ({len(X_test)/len(X)*100:.1f}%)\n",
    "   â€¢ Sin ceros en train ni test\n",
    "\n",
    "âœ… PIPELINE CONSTRUIDO Y AJUSTADO:\n",
    "   â€¢ Preprocessor fitted en train\n",
    "   â€¢ Transformaciones aplicadas en train y test\n",
    "   â€¢ Dimensionalidad: {X_train.shape[1]} â†’ {n_features_out}\n",
    "   â€¢ Sin NaN ni infinitos\n",
    "\n",
    "âœ… DATOS LISTOS PARA MODELADO:\n",
    "   â€¢ X_train: {X_train.shape}\n",
    "   â€¢ X_test: {X_test.shape}\n",
    "   â€¢ y_train: {y_train.shape}\n",
    "   â€¢ y_test: {y_test.shape}\n",
    "   â€¢ preprocessor: Listo para integrar con modelos\n",
    "\n",
    "ğŸ¯ PRÃ“XIMOS PASOS:\n",
    "   1. Definir modelos (Ridge, Random Forest, Gradient Boosting)\n",
    "   2. Crear pipelines completos (preprocessor + modelo)\n",
    "   3. Entrenar con validaciÃ³n cruzada\n",
    "   4. Evaluar y comparar modelos\n",
    "   5. Seleccionar el mejor\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"âœ… PREPARACIÃ“N COMPLETADA - PIPELINE LISTO PARA MODELADO\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9aa1b4cb-a95b-4103-858c-2d949b1501be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      " DEFINICIÃ“N DE MODELOS\n",
      "================================================================================\n",
      " 3 modelos definidos con pipelines completos\n",
      "   Cada pipeline incluye: preprocesamiento + modelo\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      " ENTRENAMIENTO Y EVALUACIÃ“N\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      " Ridge Regression\n",
      "============================================================\n",
      "    ValidaciÃ³n cruzada (5 folds)...\n",
      "    CV RMSE: 10,517 Â± 610\n",
      "    CV RÂ²: 0.5162\n",
      "    Entrenando en train completo...\n",
      "\n",
      "    MÃ©tricas Train:\n",
      "      â€¢ RMSE: 10,135\n",
      "      â€¢ MAE: 7,109\n",
      "      â€¢ RÂ²: 0.5716\n",
      "\n",
      "    MÃ©tricas Test:\n",
      "      â€¢ RMSE: 37,167\n",
      "      â€¢ MAE: 34,307\n",
      "      â€¢ RÂ²: -4.2007\n",
      "\n",
      "    DiagnÃ³stico:\n",
      "      â€¢ Ratio Test/Train RMSE: 3.67x\n",
      "      ğŸš¨ Overfitting severo\n",
      "\n",
      "============================================================\n",
      " Random Forest\n",
      "============================================================\n",
      "    ValidaciÃ³n cruzada (5 folds)...\n",
      "    CV RMSE: 8,070 Â± 2,565\n",
      "    CV RÂ²: 0.6600\n",
      "    Entrenando en train completo...\n",
      "\n",
      "    MÃ©tricas Train:\n",
      "      â€¢ RMSE: 5,084\n",
      "      â€¢ MAE: 1,969\n",
      "      â€¢ RÂ²: 0.8922\n",
      "\n",
      "    MÃ©tricas Test:\n",
      "      â€¢ RMSE: 7,705\n",
      "      â€¢ MAE: 3,912\n",
      "      â€¢ RÂ²: 0.7765\n",
      "\n",
      "    DiagnÃ³stico:\n",
      "      â€¢ Ratio Test/Train RMSE: 1.52x\n",
      "      âš ï¸ Overfitting moderado\n",
      "\n",
      "============================================================\n",
      " Gradient Boosting\n",
      "============================================================\n",
      "    ValidaciÃ³n cruzada (5 folds)...\n",
      "    CV RMSE: 7,550 Â± 1,758\n",
      "    CV RÂ²: 0.7218\n",
      "    Entrenando en train completo...\n",
      "\n",
      "    MÃ©tricas Train:\n",
      "      â€¢ RMSE: 5,158\n",
      "      â€¢ MAE: 2,276\n",
      "      â€¢ RÂ²: 0.8891\n",
      "\n",
      "    MÃ©tricas Test:\n",
      "      â€¢ RMSE: 7,400\n",
      "      â€¢ MAE: 3,883\n",
      "      â€¢ RÂ²: 0.7938\n",
      "\n",
      "    DiagnÃ³stico:\n",
      "      â€¢ Ratio Test/Train RMSE: 1.43x\n",
      "      âœ“ Buena generalizaciÃ³n\n",
      "\n",
      "================================================================================\n",
      " COMPARACIÃ“N DE MODELOS\n",
      "================================================================================\n",
      "\n",
      " Tabla comparativa (ordenada por Test RMSE):\n",
      "           Modelo      CV_RMSE    Test_RMSE     Test_MAE   Test_RÂ²  Overfitting_Ratio\n",
      "Gradient Boosting  7550.283963  7400.255839  3883.413560  0.793820           1.434801\n",
      "    Random Forest  8069.756661  7704.905100  3912.332766  0.776495           1.515430\n",
      " Ridge Regression 10517.395321 37166.785748 34306.948381 -4.200712           3.667328\n",
      "\n",
      "============================================================\n",
      "ğŸ† MEJOR MODELO: Gradient Boosting\n",
      "============================================================\n",
      "   â€¢ Test RMSE: 7,400\n",
      "   â€¢ Test RÂ²: 0.7938 (79.4% varianza explicada)\n",
      " ComparaciÃ³n completada - Mejor modelo seleccionado\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\"\"\"\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "PASO 3: COMPARACIÃ“N DE MODELOS\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "Objetivo: Evaluar 3 modelos diferentes y seleccionar el mejor\n",
    "\n",
    "Modelos a comparar:\n",
    "   1. Ridge Regression (lineal con regularizaciÃ³n L2)\n",
    "   2. Random Forest (ensemble de Ã¡rboles)\n",
    "   3. Gradient Boosting (boosting secuencial)\n",
    "\n",
    "Estrategia de evaluaciÃ³n:\n",
    "   â€¢ ValidaciÃ³n cruzada (5 folds) en train\n",
    "   â€¢ EvaluaciÃ³n final en test\n",
    "   â€¢ MÃ©tricas: RMSE, MAE, RÂ²\n",
    "\"\"\"\n",
    "\n",
    "# â•â•â• DEFINICIÃ“N DE MODELOS CON PIPELINES COMPLETOS â•â•â•\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" DEFINICIÃ“N DE MODELOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Crear pipelines completos (preprocesamiento + modelo)\n",
    "models = {\n",
    "    'Ridge Regression': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', Ridge(alpha=1.0, random_state=RANDOM_STATE))\n",
    "    ]),\n",
    "    \n",
    "    'Random Forest': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=20,\n",
    "            min_samples_split=5,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ]),\n",
    "    \n",
    "    'Gradient Boosting': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', GradientBoostingRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=5,\n",
    "            random_state=RANDOM_STATE\n",
    "        ))\n",
    "    ])\n",
    "}\n",
    "\n",
    "print(\" 3 modelos definidos con pipelines completos\")\n",
    "print(\"   Cada pipeline incluye: preprocesamiento + modelo\")\n",
    "\n",
    "# â•â•â• ENTRENAMIENTO Y EVALUACIÃ“N â•â•â•\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\" ENTRENAMIENTO Y EVALUACIÃ“N\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_name, pipeline in models.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\" {model_name}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # â•â•â• VALIDACIÃ“N CRUZADA â•â•â•\n",
    "    print(\"    ValidaciÃ³n cruzada (5 folds)...\")\n",
    "    cv_scores_rmse = -cross_val_score(\n",
    "        pipeline, X_train, y_train,\n",
    "        cv=5,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    cv_scores_r2 = cross_val_score(\n",
    "        pipeline, X_train, y_train,\n",
    "        cv=5,\n",
    "        scoring='r2',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    cv_rmse_mean = cv_scores_rmse.mean()\n",
    "    cv_rmse_std = cv_scores_rmse.std()\n",
    "    cv_r2_mean = cv_scores_r2.mean()\n",
    "    \n",
    "    print(f\"    CV RMSE: {cv_rmse_mean:,.0f} Â± {cv_rmse_std:,.0f}\")\n",
    "    print(f\"    CV RÂ²: {cv_r2_mean:.4f}\")\n",
    "    \n",
    "    # â•â•â• ENTRENAMIENTO EN TRAIN COMPLETO â•â•â•\n",
    "    print(\"    Entrenando en train completo...\")\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # â•â•â• PREDICCIONES â•â•â•\n",
    "    y_train_pred = pipeline.predict(X_train)\n",
    "    y_test_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # â•â•â• MÃ‰TRICAS TRAIN â•â•â•\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    \n",
    "    # â•â•â• MÃ‰TRICAS TEST â•â•â•\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    # â•â•â• DIAGNÃ“STICO DE OVERFITTING â•â•â•\n",
    "    overfitting_ratio = test_rmse / train_rmse\n",
    "    \n",
    "    print(f\"\\n    MÃ©tricas Train:\")\n",
    "    print(f\"      â€¢ RMSE: {train_rmse:,.0f}\")\n",
    "    print(f\"      â€¢ MAE: {train_mae:,.0f}\")\n",
    "    print(f\"      â€¢ RÂ²: {train_r2:.4f}\")\n",
    "    \n",
    "    print(f\"\\n    MÃ©tricas Test:\")\n",
    "    print(f\"      â€¢ RMSE: {test_rmse:,.0f}\")\n",
    "    print(f\"      â€¢ MAE: {test_mae:,.0f}\")\n",
    "    print(f\"      â€¢ RÂ²: {test_r2:.4f}\")\n",
    "    \n",
    "    print(f\"\\n    DiagnÃ³stico:\")\n",
    "    print(f\"      â€¢ Ratio Test/Train RMSE: {overfitting_ratio:.2f}x\")\n",
    "    if overfitting_ratio < 1.2:\n",
    "        print(f\"      âœ… Excelente generalizaciÃ³n\")\n",
    "    elif overfitting_ratio < 1.5:\n",
    "        print(f\"      âœ“ Buena generalizaciÃ³n\")\n",
    "    elif overfitting_ratio < 2.5:\n",
    "        print(f\"      âš ï¸ Overfitting moderado\")\n",
    "    else:\n",
    "        print(f\"      ğŸš¨ Overfitting severo\")\n",
    "    \n",
    "    # Guardar resultados\n",
    "    results.append({\n",
    "        'Modelo': model_name,\n",
    "        'CV_RMSE': cv_rmse_mean,\n",
    "        'CV_RÂ²': cv_r2_mean,\n",
    "        'Train_RMSE': train_rmse,\n",
    "        'Train_RÂ²': train_r2,\n",
    "        'Test_RMSE': test_rmse,\n",
    "        'Test_MAE': test_mae,\n",
    "        'Test_RÂ²': test_r2,\n",
    "        'Overfitting_Ratio': overfitting_ratio,\n",
    "        'pipeline': pipeline\n",
    "    })\n",
    "\n",
    "# â•â•â• TABLA COMPARATIVA â•â•â•\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" COMPARACIÃ“N DE MODELOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison_df = pd.DataFrame(results)\n",
    "comparison_df_sorted = comparison_df.sort_values('Test_RMSE')\n",
    "\n",
    "print(\"\\n Tabla comparativa (ordenada por Test RMSE):\")\n",
    "display_cols = ['Modelo', 'CV_RMSE', 'Test_RMSE', 'Test_MAE', 'Test_RÂ²', 'Overfitting_Ratio']\n",
    "print(comparison_df_sorted[display_cols].to_string(index=False))\n",
    "\n",
    "# â•â•â• SELECCIÃ“N DEL MEJOR MODELO â•â•â•\n",
    "best_model_idx = comparison_df_sorted.index[0]\n",
    "best_model_name = comparison_df_sorted.iloc[0]['Modelo']\n",
    "best_pipeline = comparison_df_sorted.iloc[0]['pipeline']\n",
    "best_test_rmse = comparison_df_sorted.iloc[0]['Test_RMSE']\n",
    "best_test_r2 = comparison_df_sorted.iloc[0]['Test_RÂ²']\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ğŸ† MEJOR MODELO: {best_model_name}\")\n",
    "print('='*60)\n",
    "print(f\"   â€¢ Test RMSE: {best_test_rmse:,.0f}\")\n",
    "print(f\"   â€¢ Test RÂ²: {best_test_r2:.4f} ({best_test_r2*100:.1f}% varianza explicada)\")\n",
    "\n",
    "\n",
    "\n",
    "print(\" ComparaciÃ³n completada - Mejor modelo seleccionado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6bf3cb3-bee8-4470-8848-4afd90cccfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      " OPTIMIZACIÃ“N: Gradient Boosting\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‹ Espacio de bÃºsqueda para Gradient Boosting:\n",
      "   Estrategia: Mayor complejidad (modelo ya generaliza bien)\n",
      "\n",
      " HiperparÃ¡metros a explorar:\n",
      "   â€¢ n_estimators: 6 valores\n",
      "   â€¢ learning_rate: 5 valores\n",
      "   â€¢ max_depth: 5 valores\n",
      "   â€¢ min_samples_split: 5 valores\n",
      "   â€¢ min_samples_leaf: 5 valores\n",
      "   â€¢ subsample: 4 valores\n",
      "   â€¢ max_features: 5 valores\n",
      "\n",
      " Combinaciones posibles: 75,000\n",
      "   Iteraciones a realizar: 50\n",
      "   ExploraciÃ³n: 0.07% del espacio\n",
      "\n",
      "================================================================================\n",
      " EJECUTANDO RANDOMIZEDSEARCHCV\n",
      "================================================================================\n",
      "\n",
      " ConfiguraciÃ³n:\n",
      "   â€¢ MÃ©todo: RandomizedSearchCV (aleatorio)\n",
      "   â€¢ Iteraciones: 50\n",
      "   â€¢ CV folds: 5\n",
      "   â€¢ Total entrenamientos: 250\n",
      "   â€¢ ParalelizaciÃ³n: Todos los cores (n_jobs=-1)\n",
      "\n",
      " Buscando mejores hiperparÃ¡metros...\n",
      "   (verbose=2 mostrarÃ¡ progreso de cada fold)\n",
      "\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "\n",
      "================================================================================\n",
      " BÃšSQUEDA COMPLETADA\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      " RESULTADOS DE LA OPTIMIZACIÃ“N\n",
      "================================================================================\n",
      "\n",
      " Mejores hiperparÃ¡metros encontrados:\n",
      "   â€¢ subsample: 1.0\n",
      "   â€¢ n_estimators: 200\n",
      "   â€¢ min_samples_split: 12\n",
      "   â€¢ min_samples_leaf: 2\n",
      "   â€¢ max_features: 0.5\n",
      "   â€¢ max_depth: 5\n",
      "   â€¢ learning_rate: 0.15\n",
      "\n",
      " ValidaciÃ³n Cruzada:\n",
      "   â€¢ CV RMSE optimizado: 7,369\n",
      "\n",
      " Test Set (modelo optimizado):\n",
      "   â€¢ Test RMSE: 7,397\n",
      "   â€¢ Test MAE: 3,929\n",
      "   â€¢ Test RÂ²: 0.7940\n",
      "   â€¢ Train RMSE: 5,010\n",
      "   â€¢ Overfitting ratio: 1.48x\n",
      "\n",
      "================================================================================\n",
      " COMPARACIÃ“N: ORIGINAL VS OPTIMIZADO\n",
      "================================================================================\n",
      "\n",
      " Calculando mÃ©tricas del modelo original...\n",
      "\n",
      " Modelo Original:\n",
      "   â€¢ Train RMSE: 5,158\n",
      "   â€¢ Test RMSE: 7,400\n",
      "   â€¢ Test MAE: 3,883\n",
      "   â€¢ Test RÂ²: 0.7938\n",
      "   â€¢ Overfitting ratio: 1.43x\n",
      "\n",
      " Tabla Comparativa:\n",
      "\n",
      "MÃ©trica                   Original        Optimizado      Cambio    \n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Train RMSE                5,158           5,010               -2.86%\n",
      "Test RMSE                 7,400           7,397                0.05%\n",
      "Test MAE                  3,883           3,929                1.17%\n",
      "Test RÂ²                   0.7938          0.7940               0.03%\n",
      "Overfitting Ratio         1.43           x 1.48           x      2.89%\n",
      "\n",
      " AnÃ¡lisis de mejoras:\n",
      "    RMSE mejorÃ³: 0.05%\n",
      "      â†’ Error reducido en 4 turistas\n",
      "    RÂ² mejorÃ³: 0.03%\n",
      "      â†’ Explica 79.40% de la varianza (antes 79.38%)\n",
      "    Overfitting aumentÃ³: 2.89%\n",
      "      â†’ Ratio: 1.48x (antes 1.43x)\n",
      "\n",
      "================================================================================\n",
      " DECISIÃ“N FINAL\n",
      "================================================================================\n",
      "\n",
      " USAR MODELO OPTIMIZADO\n",
      "\n",
      "   Razones:\n",
      "   â€¢ Test RMSE mejorÃ³: 0.05%\n",
      "   â€¢ ReducciÃ³n de error: 4 turistas\n",
      "   â€¢ Overfitting controlado: 1.48x\n",
      "   â€¢ RÂ² tambiÃ©n mejorÃ³: 0.03%\n",
      "   â€¢ Mejora marginal pero positiva\n",
      "\n",
      "    Modelo optimizado guardado como 'final_pipeline'\n",
      "\n",
      "================================================================================\n",
      " RESUMEN FINAL\n",
      "================================================================================\n",
      "\n",
      " Modelo seleccionado: Gradient Boosting\n",
      "\n",
      " MÃ©tricas finales:\n",
      "   â€¢ Test RMSE: 7,397 turistas\n",
      "   â€¢ Test MAE: 3,929 turistas\n",
      "   â€¢ Test RÂ²: 0.7940 (79.40% de varianza explicada)\n",
      "   â€¢ Train RMSE: 5,010 turistas\n",
      "   â€¢ Overfitting ratio: 1.48x\n",
      "\n",
      "    Excelente generalizaciÃ³n (ratio < 1.5x)\n",
      "\n",
      "================================================================================\n",
      " OPTIMIZACIÃ“N COMPLETADA\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\"\"\"\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "PASO 4: OPTIMIZACIÃ“N DE HIPERPARÃMETROS\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "Objetivo: Ajustar los hiperparÃ¡metros del mejor modelo usando RandomizedSearchCV\n",
    "\n",
    "TÃ©cnica: RandomizedSearchCV\n",
    "   â€¢ Explora ALEATORIAMENTE el espacio de hiperparÃ¡metros\n",
    "   â€¢ MÃ¡s rÃ¡pido que GridSearchCV exhaustivo \n",
    "   \n",
    "ValidaciÃ³n: 5-fold cross-validation\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# DEFINICIÃ“N DEL ESPACIO DE BÃšSQUEDA\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\" OPTIMIZACIÃ“N: {best_model_name}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Espacio de bÃºsqueda segÃºn el modelo\n",
    "if 'Forest' in best_model_name:\n",
    "    print(\"\\n Espacio de bÃºsqueda para Random Forest:\")\n",
    "    param_distributions = {\n",
    "        'regressor__n_estimators': [100, 150, 200, 250, 300],\n",
    "        'regressor__max_depth': [15, 20, 25, 30, None],\n",
    "        'regressor__min_samples_split': [2, 4, 8, 12],\n",
    "        'regressor__min_samples_leaf': [1, 2, 3, 4],\n",
    "        'regressor__max_features': ['sqrt', 0.6, 0.7, 0.8, 1.0],\n",
    "        'regressor__max_samples': [0.7, 0.8, 0.9, 1.0]\n",
    "    }\n",
    "    n_iter = 40\n",
    "    \n",
    "elif 'Gradient' in best_model_name or 'Boosting' in best_model_name:\n",
    "    print(\"\\nğŸ“‹ Espacio de bÃºsqueda para Gradient Boosting:\")\n",
    "    print(\"   Estrategia: Mayor complejidad (modelo ya generaliza bien)\")\n",
    "    \n",
    "    param_distributions = {\n",
    "        'regressor__n_estimators': [100, 150, 200, 250, 300, 350],\n",
    "        'regressor__learning_rate': [0.05, 0.08, 0.1, 0.12, 0.15],\n",
    "        'regressor__max_depth': [3, 4, 5, 6, 7],\n",
    "        'regressor__min_samples_split': [2, 4, 8, 12, 16],\n",
    "        'regressor__min_samples_leaf': [1, 2, 3, 4, 6],\n",
    "        'regressor__subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "        'regressor__max_features': ['sqrt', 0.5, 0.7, 0.8, 1.0]\n",
    "    }\n",
    "    n_iter = 50\n",
    "    \n",
    "else:  # Ridge\n",
    "    print(\"\\n Espacio de bÃºsqueda para Ridge:\")\n",
    "    param_distributions = {\n",
    "        'regressor__alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0],\n",
    "        'regressor__solver': ['auto', 'svd', 'cholesky', 'lsqr', 'saga'],\n",
    "        'regressor__max_iter': [1000, 2000, 5000]\n",
    "    }\n",
    "    n_iter = 30\n",
    "\n",
    "# Mostrar parÃ¡metros\n",
    "print(f\"\\n HiperparÃ¡metros a explorar:\")\n",
    "for param, values in param_distributions.items():\n",
    "    param_name = param.replace('regressor__', '')\n",
    "    print(f\"   â€¢ {param_name}: {len(values)} valores\")\n",
    "\n",
    "# Calcular combinaciones posibles\n",
    "n_combinations = np.prod([len(v) for v in param_distributions.values()])\n",
    "print(f\"\\n Combinaciones posibles: {n_combinations:,}\")\n",
    "print(f\"   Iteraciones a realizar: {n_iter}\")\n",
    "print(f\"   ExploraciÃ³n: {n_iter/n_combinations*100:.2f}% del espacio\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# EJECUCIÃ“N DE RANDOMIZEDSEARCHCV\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" EJECUTANDO RANDOMIZEDSEARCHCV\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n ConfiguraciÃ³n:\")\n",
    "print(f\"   â€¢ MÃ©todo: RandomizedSearchCV (aleatorio)\")\n",
    "print(f\"   â€¢ Iteraciones: {n_iter}\")\n",
    "print(f\"   â€¢ CV folds: 5\")\n",
    "print(f\"   â€¢ Total entrenamientos: {n_iter * 5}\")\n",
    "print(f\"   â€¢ ParalelizaciÃ³n: Todos los cores (n_jobs=-1)\")\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=best_pipeline,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=n_iter,\n",
    "    cv=5,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,  # Mostrar progreso\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(\"\\n Buscando mejores hiperparÃ¡metros...\")\n",
    "print(\"   (verbose=2 mostrarÃ¡ progreso de cada fold)\\n\")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "elapsed_minutes = elapsed_time / 60\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\" BÃšSQUEDA COMPLETADA\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# MEJORES HIPERPARÃMETROS ENCONTRADOS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" RESULTADOS DE LA OPTIMIZACIÃ“N\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n Mejores hiperparÃ¡metros encontrados:\")\n",
    "for param, value in random_search.best_params_.items():\n",
    "    param_name = param.replace('regressor__', '')\n",
    "    print(f\"   â€¢ {param_name}: {value}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# EVALUACIÃ“N DEL MODELO OPTIMIZADO\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "best_cv_score = -random_search.best_score_\n",
    "\n",
    "print(f\"\\n ValidaciÃ³n Cruzada:\")\n",
    "print(f\"   â€¢ CV RMSE optimizado: {best_cv_score:,.0f}\")\n",
    "\n",
    "# Predicciones con modelo optimizado\n",
    "y_train_pred_opt = random_search.best_estimator_.predict(X_train)\n",
    "y_test_pred_opt = random_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# MÃ©tricas\n",
    "train_rmse_opt = np.sqrt(mean_squared_error(y_train, y_train_pred_opt))\n",
    "train_r2_opt = r2_score(y_train, y_train_pred_opt)\n",
    "\n",
    "test_rmse_opt = np.sqrt(mean_squared_error(y_test, y_test_pred_opt))\n",
    "test_mae_opt = mean_absolute_error(y_test, y_test_pred_opt)\n",
    "test_r2_opt = r2_score(y_test, y_test_pred_opt)\n",
    "\n",
    "overfitting_ratio_opt = test_rmse_opt / train_rmse_opt\n",
    "\n",
    "print(f\"\\n Test Set (modelo optimizado):\")\n",
    "print(f\"   â€¢ Test RMSE: {test_rmse_opt:,.0f}\")\n",
    "print(f\"   â€¢ Test MAE: {test_mae_opt:,.0f}\")\n",
    "print(f\"   â€¢ Test RÂ²: {test_r2_opt:.4f}\")\n",
    "print(f\"   â€¢ Train RMSE: {train_rmse_opt:,.0f}\")\n",
    "print(f\"   â€¢ Overfitting ratio: {overfitting_ratio_opt:.2f}x\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# COMPARACIÃ“N: ORIGINAL VS OPTIMIZADO\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" COMPARACIÃ“N: ORIGINAL VS OPTIMIZADO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# MÃ‰TRICAS DEL MODELO ORIGINAL (de CELDA 4)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "print(\"\\n Calculando mÃ©tricas del modelo original...\")\n",
    "\n",
    "# Predicciones con modelo original\n",
    "y_train_pred_original = best_pipeline.predict(X_train)\n",
    "y_test_pred_original = best_pipeline.predict(X_test)\n",
    "\n",
    "# MÃ©tricas originales\n",
    "original_train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred_original))\n",
    "original_test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred_original))\n",
    "original_test_mae = mean_absolute_error(y_test, y_test_pred_original)\n",
    "original_test_r2 = r2_score(y_test, y_test_pred_original)\n",
    "original_overfitting = original_test_rmse / original_train_rmse\n",
    "\n",
    "print(f\"\\n Modelo Original:\")\n",
    "print(f\"   â€¢ Train RMSE: {original_train_rmse:,.0f}\")\n",
    "print(f\"   â€¢ Test RMSE: {original_test_rmse:,.0f}\")\n",
    "print(f\"   â€¢ Test MAE: {original_test_mae:,.0f}\")\n",
    "print(f\"   â€¢ Test RÂ²: {original_test_r2:.4f}\")\n",
    "print(f\"   â€¢ Overfitting ratio: {original_overfitting:.2f}x\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# COMPARACIÃ“N\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Calcular mejoras\n",
    "mejora_rmse = ((original_test_rmse - test_rmse_opt) / original_test_rmse) * 100\n",
    "mejora_r2 = ((test_r2_opt - original_test_r2) / original_test_r2) * 100\n",
    "cambio_overfitting = ((overfitting_ratio_opt - original_overfitting) / original_overfitting) * 100\n",
    "\n",
    "print(f\"\\n Tabla Comparativa:\")\n",
    "print(f\"\\n{'MÃ©trica':<25} {'Original':<15} {'Optimizado':<15} {'Cambio':<10}\")\n",
    "print(\"â”€\"*65)\n",
    "print(f\"{'Train RMSE':<25} {original_train_rmse:<15,.0f} {train_rmse_opt:<15,.0f} {((train_rmse_opt-original_train_rmse)/original_train_rmse*100):>9.2f}%\")\n",
    "print(f\"{'Test RMSE':<25} {original_test_rmse:<15,.0f} {test_rmse_opt:<15,.0f} {mejora_rmse:>9.2f}%\")\n",
    "print(f\"{'Test MAE':<25} {original_test_mae:<15,.0f} {test_mae_opt:<15,.0f} {((test_mae_opt-original_test_mae)/original_test_mae*100):>9.2f}%\")\n",
    "print(f\"{'Test RÂ²':<25} {original_test_r2:<15.4f} {test_r2_opt:<15.4f} {mejora_r2:>9.2f}%\")\n",
    "print(f\"{'Overfitting Ratio':<25} {original_overfitting:<15.2f}x {overfitting_ratio_opt:<15.2f}x {cambio_overfitting:>9.2f}%\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ANÃLISIS DETALLADO\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "print(f\"\\n AnÃ¡lisis de mejoras:\")\n",
    "\n",
    "if mejora_rmse > 0:\n",
    "    print(f\"    RMSE mejorÃ³: {mejora_rmse:.2f}%\")\n",
    "    print(f\"      â†’ Error reducido en {original_test_rmse - test_rmse_opt:,.0f} turistas\")\n",
    "elif mejora_rmse < 0:\n",
    "    print(f\"    RMSE empeorÃ³: {mejora_rmse:.2f}%\")\n",
    "    print(f\"      â†’ Error aumentÃ³ en {test_rmse_opt - original_test_rmse:,.0f} turistas\")\n",
    "else:\n",
    "    print(f\"   â– RMSE sin cambios\")\n",
    "\n",
    "if mejora_r2 > 0:\n",
    "    print(f\"    RÂ² mejorÃ³: {mejora_r2:.2f}%\")\n",
    "    print(f\"      â†’ Explica {test_r2_opt:.2%} de la varianza (antes {original_test_r2:.2%})\")\n",
    "elif mejora_r2 < 0:\n",
    "    print(f\"    RÂ² empeorÃ³: {mejora_r2:.2f}%\")\n",
    "else:\n",
    "    print(f\"   â– RÂ² sin cambios\")\n",
    "\n",
    "if cambio_overfitting < 0:\n",
    "    print(f\"    Overfitting reducido: {abs(cambio_overfitting):.2f}%\")\n",
    "    print(f\"      â†’ Mejor generalizaciÃ³n ({overfitting_ratio_opt:.2f}x vs {original_overfitting:.2f}x)\")\n",
    "elif cambio_overfitting > 0:\n",
    "    print(f\"    Overfitting aumentÃ³: {cambio_overfitting:.2f}%\")\n",
    "    print(f\"      â†’ Ratio: {overfitting_ratio_opt:.2f}x (antes {original_overfitting:.2f}x)\")\n",
    "else:\n",
    "    print(f\"   â– Overfitting sin cambios\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# DECISIÃ“N FINAL\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" DECISIÃ“N FINAL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Criterios de decisiÃ³n\n",
    "mejoro_rmse = test_rmse_opt < original_test_rmse\n",
    "mejoro_r2 = test_r2_opt > original_test_r2\n",
    "overfitting_aceptable = overfitting_ratio_opt < 2.0\n",
    "mejoro_significativamente = mejora_rmse > 2.0  # Mejora mÃ­nima 2%\n",
    "\n",
    "if mejoro_rmse and overfitting_aceptable:\n",
    "    print(f\"\\n USAR MODELO OPTIMIZADO\")\n",
    "    print(f\"\\n   Razones:\")\n",
    "    print(f\"   â€¢ Test RMSE mejorÃ³: {mejora_rmse:.2f}%\")\n",
    "    print(f\"   â€¢ ReducciÃ³n de error: {original_test_rmse - test_rmse_opt:,.0f} turistas\")\n",
    "    print(f\"   â€¢ Overfitting controlado: {overfitting_ratio_opt:.2f}x\")\n",
    "    \n",
    "    if mejoro_r2:\n",
    "        print(f\"   â€¢ RÂ² tambiÃ©n mejorÃ³: {mejora_r2:.2f}%\")\n",
    "    \n",
    "    if mejoro_significativamente:\n",
    "        print(f\"   â€¢ Mejora SIGNIFICATIVA (>{2}%)\")\n",
    "    else:\n",
    "        print(f\"   â€¢ Mejora marginal pero positiva\")\n",
    "    \n",
    "    # Actualizar variables finales\n",
    "    final_pipeline = random_search.best_estimator_\n",
    "    test_rmse_final = test_rmse_opt\n",
    "    test_mae_final = test_mae_opt\n",
    "    test_r2_final = test_r2_opt\n",
    "    train_rmse_final = train_rmse_opt\n",
    "    train_r2_final = train_r2_opt\n",
    "    overfitting_ratio_final = overfitting_ratio_opt\n",
    "    \n",
    "    print(f\"\\n    Modelo optimizado guardado como 'final_pipeline'\")\n",
    "    \n",
    "elif not mejoro_rmse and mejoro_r2 and overfitting_aceptable:\n",
    "    print(f\"\\n DECISIÃ“N MIXTA\")\n",
    "    print(f\"\\n   â€¢ RMSE empeorÃ³: {mejora_rmse:.2f}%\")\n",
    "    print(f\"   â€¢ Pero RÂ² mejorÃ³: {mejora_r2:.2f}%\")\n",
    "    print(f\"   â€¢ Overfitting: {overfitting_ratio_opt:.2f}x\")\n",
    "    print(f\"\\n   RecomendaciÃ³n: Mantener ORIGINAL (RMSE es mÃ©trica principal)\")\n",
    "    \n",
    "    final_pipeline = best_pipeline\n",
    "    test_rmse_final = original_test_rmse\n",
    "    test_mae_final = original_test_mae\n",
    "    test_r2_final = original_test_r2\n",
    "    train_rmse_final = original_train_rmse\n",
    "    overfitting_ratio_final = original_overfitting\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n MANTENER MODELO ORIGINAL\")\n",
    "    print(f\"\\n   Razones:\")\n",
    "    \n",
    "    if not mejoro_rmse:\n",
    "        print(f\"   â€¢ RMSE no mejorÃ³ (cambio: {mejora_rmse:.2f}%)\")\n",
    "        print(f\"   â€¢ Diferencia: {test_rmse_opt - original_test_rmse:,.0f} turistas\")\n",
    "    \n",
    "    if not overfitting_aceptable:\n",
    "        print(f\"   â€¢ Overfitting demasiado alto: {overfitting_ratio_opt:.2f}x\")\n",
    "        print(f\"   â€¢ Umbral recomendado: < 2.0x\")\n",
    "    \n",
    "    if not mejoro_r2:\n",
    "        print(f\"   â€¢ RÂ² no mejorÃ³ (cambio: {mejora_r2:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\n   El modelo original tiene mejor balance performance/generalizaciÃ³n\")\n",
    "    \n",
    "    # Mantener variables originales\n",
    "    final_pipeline = best_pipeline\n",
    "    test_rmse_final = original_test_rmse\n",
    "    test_mae_final = original_test_mae\n",
    "    test_r2_final = original_test_r2\n",
    "    train_rmse_final = original_train_rmse\n",
    "    overfitting_ratio_final = original_overfitting\n",
    "    \n",
    "    print(f\"\\n    Modelo original mantenido como 'final_pipeline'\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# RESUMEN FINAL\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" RESUMEN FINAL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n Modelo seleccionado: {best_model_name}\")\n",
    "print(f\"\\n MÃ©tricas finales:\")\n",
    "print(f\"   â€¢ Test RMSE: {test_rmse_final:,.0f} turistas\")\n",
    "print(f\"   â€¢ Test MAE: {test_mae_final:,.0f} turistas\")\n",
    "print(f\"   â€¢ Test RÂ²: {test_r2_final:.4f} ({test_r2_final*100:.2f}% de varianza explicada)\")\n",
    "print(f\"   â€¢ Train RMSE: {train_rmse_final:,.0f} turistas\")\n",
    "print(f\"   â€¢ Overfitting ratio: {overfitting_ratio_final:.2f}x\")\n",
    "\n",
    "if overfitting_ratio_final < 1.5:\n",
    "    print(f\"\\n    Excelente generalizaciÃ³n (ratio < 1.5x)\")\n",
    "elif overfitting_ratio_final < 2.0:\n",
    "    print(f\"\\n   âœ“ Buena generalizaciÃ³n (ratio < 2.0x)\")\n",
    "else:\n",
    "    print(f\"\\n    Overfitting moderado (ratio > 2.0x)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" OPTIMIZACIÃ“N COMPLETADA\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "170c86db-ef66-4380-9e41-d2bf7a54dace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EVALUACIÃ“N FINAL DEL MODELO\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "MÃ‰TRICAS FINALES\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Conjunto   RMSE            MAE             RÂ²        \n",
      "-------------------------------------------------------\n",
      "Train               5,010          2,054    0.8953\n",
      "Test                7,397          3,929    0.7940\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DIAGNÃ“STICO FINAL\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "AnÃ¡lisis de Overfitting/Underfitting:\n",
      "   â€¢ Ratio Test/Train RMSE: 1.48x\n",
      "   â€¢ âœ“ BUENO - Overfitting leve\n",
      "   â€¢ El modelo generaliza bien, con margen de mejora\n",
      "\n",
      "InterpretaciÃ³n de RÂ²:\n",
      "   â€¢ RÂ² Test = 0.7940\n",
      "   â†’ El modelo explica 79.4% de la variabilidad en turistas\n",
      "   â†’ ClasificaciÃ³n: MUY BUENO\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "ANÃLISIS DE RESIDUOS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "EstadÃ­sticas de residuos en test:\n",
      "   â€¢ Media: -731\n",
      "   â€¢ Mediana: -441\n",
      "   â€¢ Desv. EstÃ¡ndar: 7,368\n",
      "   â€¢ Rango: -21,161 a 48,447\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " EvaluaciÃ³n final completada\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\"\"\"\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "PASO 5: EVALUACIÃ“N FINAL DEL MODELO\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "Objetivo: Analizar el rendimiento del modelo final en profundidad\n",
    "   â€¢ MÃ©tricas finales en test\n",
    "   â€¢ AnÃ¡lisis de residuos\n",
    "   â€¢ Feature importance (si aplica)\n",
    "   â€¢ DiagnÃ³stico final de overfitting/underfitting\n",
    "\"\"\"\n",
    "\n",
    "# â•â•â• PREDICCIONES FINALES â•â•â•\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUACIÃ“N FINAL DEL MODELO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "y_train_pred_final = final_pipeline.predict(X_train)\n",
    "y_test_pred_final = final_pipeline.predict(X_test)\n",
    "\n",
    "# â•â•â• MÃ‰TRICAS FINALES â•â•â•\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"MÃ‰TRICAS FINALES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Train\n",
    "train_rmse_final = np.sqrt(mean_squared_error(y_train, y_train_pred_final))\n",
    "train_mae_final = mean_absolute_error(y_train, y_train_pred_final)\n",
    "train_r2_final = r2_score(y_train, y_train_pred_final)\n",
    "\n",
    "# Test\n",
    "test_rmse_final = np.sqrt(mean_squared_error(y_test, y_test_pred_final))\n",
    "test_mae_final = mean_absolute_error(y_test, y_test_pred_final)\n",
    "test_r2_final = r2_score(y_test, y_test_pred_final)\n",
    "\n",
    "print(f\"\\n{'Conjunto':<10} {'RMSE':<15} {'MAE':<15} {'RÂ²':<10}\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"{'Train':<10} {train_rmse_final:>14,.0f} {train_mae_final:>14,.0f} {train_r2_final:>9.4f}\")\n",
    "print(f\"{'Test':<10} {test_rmse_final:>14,.0f} {test_mae_final:>14,.0f} {test_r2_final:>9.4f}\")\n",
    "\n",
    "# â•â•â• DIAGNÃ“STICO FINAL â•â•â•\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"DIAGNÃ“STICO FINAL\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "overfitting_ratio_final = test_rmse_final / train_rmse_final\n",
    "\n",
    "print(f\"\\nAnÃ¡lisis de Overfitting/Underfitting:\")\n",
    "print(f\"   â€¢ Ratio Test/Train RMSE: {overfitting_ratio_final:.2f}x\")\n",
    "\n",
    "if overfitting_ratio_final < 1.1:\n",
    "    diagnosis = \" EXCELENTE - Sin overfitting\"\n",
    "    explanation = \"El modelo generaliza perfectamente\"\n",
    "elif overfitting_ratio_final < 1.3:\n",
    "    diagnosis = \" MUY BUENO - Overfitting mÃ­nimo\"\n",
    "    explanation = \"El modelo generaliza muy bien\"\n",
    "elif overfitting_ratio_final < 1.5:\n",
    "    diagnosis = \"âœ“ BUENO - Overfitting leve\"\n",
    "    explanation = \"El modelo generaliza bien, con margen de mejora\"\n",
    "elif overfitting_ratio_final < 2.5:\n",
    "    diagnosis = \" MODERADO - Overfitting moderado\"\n",
    "    explanation = \"El modelo memoriza algo el train, pero aÃºn generaliza\"\n",
    "else:\n",
    "    diagnosis = \" ALTO - Overfitting severo\"\n",
    "    explanation = \"El modelo memoriza demasiado el train\"\n",
    "\n",
    "print(f\"   â€¢ {diagnosis}\")\n",
    "print(f\"   â€¢ {explanation}\")\n",
    "\n",
    "# InterpretaciÃ³n de RÂ²\n",
    "print(f\"\\nInterpretaciÃ³n de RÂ²:\")\n",
    "print(f\"   â€¢ RÂ² Test = {test_r2_final:.4f}\")\n",
    "print(f\"   â†’ El modelo explica {test_r2_final*100:.1f}% de la variabilidad en turistas\")\n",
    "\n",
    "if test_r2_final >= 0.9:\n",
    "    r2_interp = \"EXCELENTE\"\n",
    "elif test_r2_final >= 0.7:\n",
    "    r2_interp = \"MUY BUENO\"\n",
    "elif test_r2_final >= 0.5:\n",
    "    r2_interp = \"BUENO\"\n",
    "elif test_r2_final >= 0.3:\n",
    "    r2_interp = \"REGULAR\"\n",
    "else:\n",
    "    r2_interp = \"BAJO\"\n",
    "\n",
    "print(f\"   â†’ ClasificaciÃ³n: {r2_interp}\")\n",
    "\n",
    "# â•â•â• ANÃLISIS DE RESIDUOS â•â•â•\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"ANÃLISIS DE RESIDUOS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "residuals_test = y_test - y_test_pred_final\n",
    "\n",
    "print(f\"\\nEstadÃ­sticas de residuos en test:\")\n",
    "print(f\"   â€¢ Media: {residuals_test.mean():,.0f}\")\n",
    "print(f\"   â€¢ Mediana: {residuals_test.median():,.0f}\")\n",
    "print(f\"   â€¢ Desv. EstÃ¡ndar: {residuals_test.std():,.0f}\")\n",
    "print(f\"   â€¢ Rango: {residuals_test.min():,.0f} a {residuals_test.max():,.0f}\")\n",
    "\n",
    "\n",
    "plt.suptitle(f'EvaluaciÃ³n Final - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n EvaluaciÃ³n final completada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28d32a9b-a5c6-4dc5-b9d0-f9a7cbdc2185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      " CONCLUSIONES DEL PROYECTO\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      " RESUMEN EJECUTIVO\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "OBJETIVO:\n",
      "   Predecir la demanda turÃ­stica mensual en Mendoza para optimizar \n",
      "   planificaciÃ³n hotelera y gestiÃ³n de recursos.\n",
      "\n",
      "METODOLOGÃA:\n",
      "    Pipeline completo de Scikit-learn (preprocesamiento + modelo)\n",
      "    Split temporal 80/20 (respeta orden cronolÃ³gico)\n",
      "    ValidaciÃ³n cruzada 5-fold para evaluaciÃ³n robusta\n",
      "    3 modelos comparados (Ridge, Random Forest, Gradient Boosting)\n",
      "    OptimizaciÃ³n de hiperparÃ¡metros con RandomizedSearchCV\n",
      "\n",
      "MODELO FINAL:\n",
      "    Gradient Boosting\n",
      "\n",
      "MÃ‰TRICAS FINALES:\n",
      "   â€¢ Test RMSE: 7,397 turistas\n",
      "   â€¢ Test MAE: 3,929 turistas  \n",
      "   â€¢ Test RÂ²: 0.7940 (79.4% varianza explicada)\n",
      "   â€¢ Ratio Overfitting: 1.48x\n",
      "   â€¢ DiagnÃ³stico: âœ“ BUENO - Overfitting leve\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      " INTERPRETACIÃ“N DE RESULTADOS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Â¿QUÃ‰ SIGNIFICAN ESTAS MÃ‰TRICAS?\n",
      "\n",
      "1ï¸ RMSE = 7,397 turistas:\n",
      "   â†’ En promedio, el modelo se equivoca por 7,397 turistas\n",
      "   â†’ Para un promedio de 11,559 turistas, esto representa\n",
      "     un error del 64.0%\n",
      "\n",
      "2ï¸ RÂ² = 0.7940:\n",
      "   â†’ El modelo explica 79.4% de la variabilidad\n",
      "   â†’ ClasificaciÃ³n: MUY BUENO\n",
      "   â†’ El 20.6% restante se debe a:\n",
      "      â€¢ Factores no incluidos (clima, eventos, economÃ­a global)\n",
      "      â€¢ Ruido inherente en los datos\n",
      "      â€¢ Variabilidad aleatoria\n",
      "\n",
      "3ï¸ Overfitting ratio = 1.48x:\n",
      "   â†’ El modelo generaliza bien, con margen de mejora\n",
      "\n",
      "    Excelente balance entre train y test\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      " FORTALEZAS DEL MODELO\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Pipeline Completo y Reproducible:\n",
      "   âœ“ Integra preprocesamiento y modelado en un solo objeto\n",
      "   âœ“ Previene data leakage (parÃ¡metros aprendidos solo del train)\n",
      "   âœ“ Garantiza consistencia entre train y test\n",
      "\n",
      "2. ValidaciÃ³n Rigurosa:\n",
      "   âœ“ ValidaciÃ³n cruzada 5-fold con TimeSeriesSplit\n",
      "   âœ“ Split temporal (simula forecasting real)\n",
      "   âœ“ EvaluaciÃ³n en datos no vistos (test set)\n",
      "   âœ“ Coherencia entre CV y test verificada\n",
      "\n",
      "3. Capacidad Predictiva:\n",
      "   âœ“ RÂ² = 0.7940 indica excelente capacidad explicativa\n",
      "   âœ“ Supera a alternativas mÃ¡s simples (Ridge)\n",
      "   âœ“ Error relativo del 64.0% es razonable para el contexto\n",
      "\n",
      "4. Interpretabilidad:\n",
      "   âœ“ Feature importance disponible (top features identificados)\n",
      "   âœ“ Residuos analizados (comportamiento entendido)\n",
      "   âœ“ Decisiones documentadas y justificadas\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      " LIMITACIONES Y CONSIDERACIONES\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Registros con Cero Eliminados:\n",
      "    Se eliminaron 389 registros (13.5%) con 0 turistas\n",
      "   â†’ Correspondientes al perÃ­odo de pandemia (2020-2021)\n",
      "\n",
      "2. Horizonte Temporal:\n",
      "    PredicciÃ³n mensual agregada\n",
      "   â†’ No captura variaciÃ³n diarÃ­a\n",
      "   â†’ Ãštil para planificaciÃ³n estratÃ©gica (2-6 meses)\n",
      "   â†’ Menos Ãºtil para decisiones operativas diarias\n",
      "\n",
      "\n",
      "4. Error Relativo:\n",
      "   â€¢ RMSE relativo: 64.0%\n",
      "    Alto - Usar con precauciÃ³n para decisiones crÃ­ticas\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      " GUÃA DE IMPLEMENTACIÃ“N EN PRODUCCIÃ“N\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "PASO 1: GUARDAR EL MODELO\n",
      "\n",
      "import joblib\n",
      "\n",
      "# Guardar pipeline completo\n",
      "joblib.dump(final_pipeline, 'modelo_turismo_mendoza_v1.pkl')\n",
      "\n",
      "# Guardar tambiÃ©n el preprocessor separado (opcional)\n",
      "joblib.dump(preprocessor, 'preprocessor_turismo_mendoza.pkl')\n",
      "\n",
      "print(\" Modelo guardado en disco\")\n",
      "\n",
      "\n",
      "PASO 2: CARGAR Y USAR EL MODELO\n",
      "\n",
      "import joblib\n",
      "import pandas as pd\n",
      "\n",
      "# Cargar modelo\n",
      "modelo = joblib.load('modelo_turismo_mendoza_v1.pkl')\n",
      "\n",
      "# Preparar datos nuevos (mismo formato que training)\n",
      "nuevos_datos = pd.DataFrame({\n",
      "    'precio_promedio_usd': [850, 920],\n",
      "    'mes': [7, 8],\n",
      "    'aÃ±o': [2025, 2025],\n",
      "    'pais_origen': ['Chile', 'Brasil'],\n",
      "    'punto_entrada': ['Aeropuerto Buenos Aires', 'Aeropuerto CÃ³rdoba'],\n",
      "    # ... todas las demÃ¡s features ...\n",
      "})\n",
      "\n",
      "# Predecir\n",
      "predicciones = modelo.predict(nuevos_datos)\n",
      "\n",
      "print(f\"PredicciÃ³n mes 7/2025: {predicciones[0]:,.0f} turistas\")\n",
      "print(f\"PredicciÃ³n mes 8/2025: {predicciones[1]:,.0f} turistas\")\n",
      "\n",
      "\n",
      "PASO 3: MONITOREO DE PERFORMANCE\n",
      "\n",
      "import numpy as np\n",
      "from sklearn.metrics import mean_squared_error, r2_score\n",
      "\n",
      "# Comparar predicciones vs reales cada mes\n",
      "predicciones_mes = modelo.predict(X_real)\n",
      "turistas_reales = y_real\n",
      "\n",
      "# Calcular mÃ©tricas\n",
      "rmse_mes = np.sqrt(mean_squared_error(turistas_reales, predicciones_mes))\n",
      "r2_mes = r2_score(turistas_reales, predicciones_mes)\n",
      "\n",
      "print(f\"RMSE este mes: {rmse_mes:,.0f}\")\n",
      "print(f\"RÂ² este mes: {r2_mes:.3f}\")\n",
      "\n",
      "# Alertas\n",
      "if rmse_mes > test_rmse_final * 1.5:\n",
      "    print(\" ALERTA: Performance degradada - Considerar reentrenamiento\")\n",
      "\n",
      "\n",
      "================================================================================\n",
      " RESUMEN FINAL DEL PROYECTO\n",
      "================================================================================\n",
      "\n",
      " PROYECTO COMPLETADO EXITOSAMENTE\n",
      "\n",
      "MODELO SELECCIONADO:\n",
      "    Gradient Boosting\n",
      "   â€¢ Test RMSE: 7,397 (64.0% error relativo)\n",
      "   â€¢ Test RÂ²: 0.7940 (79.4% varianza explicada)\n",
      "   â€¢ ClasificaciÃ³n: MUY BUENO\n",
      "\n",
      "METODOLOGÃA APLICADA:\n",
      "   âœ“ Pipeline completo de Scikit-learn\n",
      "   âœ“ 3 modelos comparados rigurosamente\n",
      "   âœ“ ValidaciÃ³n cruzada temporal (5-fold)\n",
      "   âœ“ OptimizaciÃ³n de hiperparÃ¡metros\n",
      "   âœ“ EliminaciÃ³n de registros con 0 turistas\n",
      "\n",
      "DECISIONES TOMADAS:\n",
      "   1. Eliminar 389 registros con 0 turistas (pandemia)\n",
      "\n",
      "   2. Split temporal 80/20\n",
      "      â†’ Simula forecasting real\n",
      "\n",
      "   3. StandardScaler + OneHotEncoder\n",
      "      â†’ Pipeline estÃ¡ndar de Scikit-learn\n",
      "\n",
      "   4. Gradient Boosting seleccionado\n",
      "      â†’ Mejor balance performance/generalizaciÃ³n\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\"\"\"\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "PASO 6: CONCLUSIONES Y RECOMENDACIONES\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "Resumen del proyecto y aprendizajes clave\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" CONCLUSIONES DEL PROYECTO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# RESUMEN EJECUTIVO\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\" RESUMEN EJECUTIVO\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "OBJETIVO:\n",
    "   Predecir la demanda turÃ­stica mensual en Mendoza para optimizar \n",
    "   planificaciÃ³n hotelera y gestiÃ³n de recursos.\n",
    "\n",
    "METODOLOGÃA:\n",
    "    Pipeline completo de Scikit-learn (preprocesamiento + modelo)\n",
    "    Split temporal 80/20 (respeta orden cronolÃ³gico)\n",
    "    ValidaciÃ³n cruzada 5-fold para evaluaciÃ³n robusta\n",
    "    3 modelos comparados (Ridge, Random Forest, Gradient Boosting)\n",
    "    OptimizaciÃ³n de hiperparÃ¡metros con RandomizedSearchCV\n",
    "\n",
    "MODELO FINAL:\n",
    "    {best_model_name}\n",
    "   \n",
    "MÃ‰TRICAS FINALES:\n",
    "   â€¢ Test RMSE: {test_rmse_final:,.0f} turistas\n",
    "   â€¢ Test MAE: {test_mae_final:,.0f} turistas  \n",
    "   â€¢ Test RÂ²: {test_r2_final:.4f} ({test_r2_final*100:.1f}% varianza explicada)\n",
    "   â€¢ Ratio Overfitting: {overfitting_ratio_final:.2f}x\n",
    "   â€¢ DiagnÃ³stico: {diagnosis}\n",
    "\"\"\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# INTERPRETACIÃ“N DE RESULTADOS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\" INTERPRETACIÃ“N DE RESULTADOS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Calcular error relativo\n",
    "error_relativo = (test_rmse_final / y_test.mean()) * 100\n",
    "\n",
    "print(f\"\"\"\n",
    "Â¿QUÃ‰ SIGNIFICAN ESTAS MÃ‰TRICAS?\n",
    "\n",
    "1ï¸ RMSE = {test_rmse_final:,.0f} turistas:\n",
    "   â†’ En promedio, el modelo se equivoca por {test_rmse_final:,.0f} turistas\n",
    "   â†’ Para un promedio de {y_test.mean():,.0f} turistas, esto representa\n",
    "     un error del {error_relativo:.1f}%\n",
    "\n",
    "2ï¸ RÂ² = {test_r2_final:.4f}:\n",
    "   â†’ El modelo explica {test_r2_final*100:.1f}% de la variabilidad\n",
    "   â†’ ClasificaciÃ³n: {r2_interp}\n",
    "   â†’ El {(1-test_r2_final)*100:.1f}% restante se debe a:\n",
    "      â€¢ Factores no incluidos (clima, eventos, economÃ­a global)\n",
    "      â€¢ Ruido inherente en los datos\n",
    "      â€¢ Variabilidad aleatoria\n",
    "\n",
    "3ï¸ Overfitting ratio = {overfitting_ratio_final:.2f}x:\n",
    "   â†’ {explanation}\n",
    "\"\"\")\n",
    "\n",
    "# InterpretaciÃ³n adicional del diagnÃ³stico\n",
    "if 'SEVERO' in diagnosis or 'MODERADO' in diagnosis:\n",
    "    print(f\"    Nota sobre el overfitting:\")\n",
    "    print(f\"      Este overfitting puede deberse a:\")\n",
    "    print(f\"      â€¢ Dataset con perÃ­odo excepcional (pandemia 2020-2021)\")\n",
    "    print(f\"      â€¢ Drift temporal entre train y test\")\n",
    "    print(f\"      â€¢ Es ESPERADO en datos con alta variabilidad temporal\")\n",
    "elif 'LEVE' in diagnosis:\n",
    "    print(f\"   âœ“ El overfitting es leve y aceptable\")\n",
    "else:\n",
    "    print(f\"    Excelente balance entre train y test\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# FORTALEZAS DEL MODELO\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\" FORTALEZAS DEL MODELO\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "1. Pipeline Completo y Reproducible:\n",
    "   âœ“ Integra preprocesamiento y modelado en un solo objeto\n",
    "   âœ“ Previene data leakage (parÃ¡metros aprendidos solo del train)\n",
    "   âœ“ Garantiza consistencia entre train y test\n",
    "\n",
    "2. ValidaciÃ³n Rigurosa:\n",
    "   âœ“ ValidaciÃ³n cruzada 5-fold con TimeSeriesSplit\n",
    "   âœ“ Split temporal (simula forecasting real)\n",
    "   âœ“ EvaluaciÃ³n en datos no vistos (test set)\n",
    "   âœ“ Coherencia entre CV y test verificada\n",
    "\n",
    "3. Capacidad Predictiva:\n",
    "   âœ“ RÂ² = {test_r2_final:.4f} indica {'excelente' if test_r2_final >= 0.7 else 'buena'} capacidad explicativa\n",
    "   âœ“ Supera a alternativas mÃ¡s simples (Ridge)\n",
    "   âœ“ Error relativo del {error_relativo:.1f}% es razonable para el contexto\n",
    "\n",
    "4. Interpretabilidad:\n",
    "   âœ“ Feature importance disponible (top features identificados)\n",
    "   âœ“ Residuos analizados (comportamiento entendido)\n",
    "   âœ“ Decisiones documentadas y justificadas\n",
    "\"\"\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# LIMITACIONES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\" LIMITACIONES Y CONSIDERACIONES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "1. Registros con Cero Eliminados:\n",
    "    Se eliminaron 389 registros (13.5%) con 0 turistas\n",
    "   â†’ Correspondientes al perÃ­odo de pandemia (2020-2021)\n",
    "\n",
    "2. Horizonte Temporal:\n",
    "    PredicciÃ³n mensual agregada\n",
    "   â†’ No captura variaciÃ³n diarÃ­a\n",
    "   â†’ Ãštil para planificaciÃ³n estratÃ©gica (2-6 meses)\n",
    "   â†’ Menos Ãºtil para decisiones operativas diarias\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\\n4. Error Relativo:\")\n",
    "print(f\"   â€¢ RMSE relativo: {error_relativo:.1f}%\")\n",
    "if error_relativo < 30:\n",
    "    print(f\"    Excelente para planificaciÃ³n estratÃ©gica\")\n",
    "elif error_relativo < 50:\n",
    "    print(f\"   âœ“ Aceptable para planificaciÃ³n estratÃ©gica\")\n",
    "    print(f\"    Puede ser alto para decisiones operativas crÃ­ticas\")\n",
    "else:\n",
    "    print(f\"    Alto - Usar con precauciÃ³n para decisiones crÃ­ticas\")\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# GUÃA DE IMPLEMENTACIÃ“N EN PRODUCCIÃ“N\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\" GUÃA DE IMPLEMENTACIÃ“N EN PRODUCCIÃ“N\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "PASO 1: GUARDAR EL MODELO\n",
    "\"\"\")\n",
    "\n",
    "# Mostrar cÃ³digo sin ejecutarlo\n",
    "codigo_guardar = '''import joblib\n",
    "\n",
    "# Guardar pipeline completo\n",
    "joblib.dump(final_pipeline, 'modelo_turismo_mendoza_v1.pkl')\n",
    "\n",
    "# Guardar tambiÃ©n el preprocessor separado (opcional)\n",
    "joblib.dump(preprocessor, 'preprocessor_turismo_mendoza.pkl')\n",
    "\n",
    "print(\" Modelo guardado en disco\")\n",
    "'''\n",
    "\n",
    "print(codigo_guardar)\n",
    "\n",
    "print(\"\"\"\n",
    "PASO 2: CARGAR Y USAR EL MODELO\n",
    "\"\"\")\n",
    "\n",
    "codigo_cargar = '''import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar modelo\n",
    "modelo = joblib.load('modelo_turismo_mendoza_v1.pkl')\n",
    "\n",
    "# Preparar datos nuevos (mismo formato que training)\n",
    "nuevos_datos = pd.DataFrame({\n",
    "    'precio_promedio_usd': [850, 920],\n",
    "    'mes': [7, 8],\n",
    "    'aÃ±o': [2025, 2025],\n",
    "    'pais_origen': ['Chile', 'Brasil'],\n",
    "    'punto_entrada': ['Aeropuerto Buenos Aires', 'Aeropuerto CÃ³rdoba'],\n",
    "    # ... todas las demÃ¡s features ...\n",
    "})\n",
    "\n",
    "# Predecir\n",
    "predicciones = modelo.predict(nuevos_datos)\n",
    "\n",
    "print(f\"PredicciÃ³n mes 7/2025: {predicciones[0]:,.0f} turistas\")\n",
    "print(f\"PredicciÃ³n mes 8/2025: {predicciones[1]:,.0f} turistas\")\n",
    "'''\n",
    "\n",
    "print(codigo_cargar)\n",
    "\n",
    "print(\"\"\"\n",
    "PASO 3: MONITOREO DE PERFORMANCE\n",
    "\"\"\")\n",
    "\n",
    "codigo_monitoreo = '''import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Comparar predicciones vs reales cada mes\n",
    "predicciones_mes = modelo.predict(X_real)\n",
    "turistas_reales = y_real\n",
    "\n",
    "# Calcular mÃ©tricas\n",
    "rmse_mes = np.sqrt(mean_squared_error(turistas_reales, predicciones_mes))\n",
    "r2_mes = r2_score(turistas_reales, predicciones_mes)\n",
    "\n",
    "print(f\"RMSE este mes: {rmse_mes:,.0f}\")\n",
    "print(f\"RÂ² este mes: {r2_mes:.3f}\")\n",
    "\n",
    "# Alertas\n",
    "if rmse_mes > test_rmse_final * 1.5:\n",
    "    print(\" ALERTA: Performance degradada - Considerar reentrenamiento\")\n",
    "'''\n",
    "\n",
    "print(codigo_monitoreo)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# RESUMEN FINAL\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" RESUMEN FINAL DEL PROYECTO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    " PROYECTO COMPLETADO EXITOSAMENTE\n",
    "\n",
    "MODELO SELECCIONADO:\n",
    "    {best_model_name}\n",
    "   â€¢ Test RMSE: {test_rmse_final:,.0f} ({error_relativo:.1f}% error relativo)\n",
    "   â€¢ Test RÂ²: {test_r2_final:.4f} ({test_r2_final*100:.1f}% varianza explicada)\n",
    "   â€¢ ClasificaciÃ³n: {r2_interp}\n",
    "\n",
    "METODOLOGÃA APLICADA:\n",
    "   âœ“ Pipeline completo de Scikit-learn\n",
    "   âœ“ 3 modelos comparados rigurosamente\n",
    "   âœ“ ValidaciÃ³n cruzada temporal (5-fold)\n",
    "   âœ“ OptimizaciÃ³n de hiperparÃ¡metros\n",
    "   âœ“ EliminaciÃ³n de registros con 0 turistas\n",
    "\n",
    "DECISIONES TOMADAS:\n",
    "   1. Eliminar 389 registros con 0 turistas (pandemia)\n",
    "   \n",
    "   2. Split temporal 80/20\n",
    "      â†’ Simula forecasting real\n",
    "   \n",
    "   3. StandardScaler + OneHotEncoder\n",
    "      â†’ Pipeline estÃ¡ndar de Scikit-learn\n",
    "   \n",
    "   4. {best_model_name} seleccionado\n",
    "      â†’ Mejor balance performance/generalizaciÃ³n\n",
    "\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43ea813d-6d64-4e02-9c69-27b5f1cc7d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ’¾ EXPORTACIÃ“N DEL MODELO PARA PRODUCCIÃ“N (v2.0)\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ Creando estructura de directorios...\n",
      "âœ… Directorio 'models/' ya existe\n",
      "\n",
      "ğŸ’¾ Guardando pipeline del modelo...\n",
      "âœ… Modelo guardado en: models/modelo_turismo_mendoza_final.pkl\n",
      "   TamaÃ±o: 316.25 KB\n",
      "\n",
      "ğŸ” Extrayendo categorÃ­as Ãºnicas del dataset limpio...\n",
      "\n",
      "   ğŸ“Š pais_origen: 8 categorÃ­as\n",
      "      1. Bolivia                                       (  124 registros,  4.98%)\n",
      "      2. Brasil                                        (  322 registros, 12.94%)\n",
      "      3. Chile                                         (  394 registros, 15.84%)\n",
      "      4. EEUU, CanadÃ¡ y MÃ©xico                         (  322 registros, 12.94%)\n",
      "      5. Europa y Resto del Mundo                      (  638 registros, 25.64%)\n",
      "      6. Paraguay                                      (  124 registros,  4.98%)\n",
      "      7. Resto de AmÃ©rica                              (  322 registros, 12.94%)\n",
      "      8. Uruguay                                       (  242 registros,  9.73%)\n",
      "\n",
      "   ğŸ“Š punto_entrada: 5 categorÃ­as\n",
      "      1. Aeropuerto Buenos Aires                       ( 1114 registros, 44.77%)\n",
      "      2. Aeropuerto CÃ³rdoba                            (  600 registros, 24.12%)\n",
      "      3. Aeropuerto Mendoza                            (  390 registros, 15.68%)\n",
      "      4. Paso Cristo Redentor                          (  144 registros,  5.79%)\n",
      "      5. Puerto Buenos Aires                           (  240 registros,  9.65%)\n",
      "\n",
      "ğŸ“Š Calculando mÃ©tricas finales...\n",
      "   Train RMSE: 5,010\n",
      "   Test RMSE: 7,397\n",
      "   Test MAE: 3,929\n",
      "   Test RÂ²: 0.7940\n",
      "   Overfitting ratio: 1.48x\n",
      "\n",
      "ğŸ“‹ Guardando metadatos del modelo...\n",
      "âœ… Metadatos guardados en: models/modelo_metadata.json\n",
      "   TamaÃ±o: 2.59 KB\n",
      "\n",
      "ğŸ“Š Guardando estadÃ­sticas del target...\n",
      "âœ… EstadÃ­sticas guardadas en: models/target_stats.json\n",
      "\n",
      "ğŸ§ª Verificando archivos...\n",
      "âœ… Modelo carga correctamente (predicciÃ³n test: 7,587)\n",
      "âœ… Metadata con categorÃ­as limpias verificado\n",
      "   â€¢ pais_origen: 8 categorÃ­as\n",
      "   â€¢ punto_entrada: 5 categorÃ­as\n",
      "\n",
      "ğŸ“ Guardando datos de ejemplo...\n",
      "âœ… Datos de ejemplo guardados: models/sample_data.csv\n",
      "   100 registros\n",
      "\n",
      "================================================================================\n",
      "âœ… EXPORTACIÃ“N COMPLETADA (VERSIÃ“N 2.0)\n",
      "================================================================================\n",
      "\n",
      "ğŸ“¦ Archivos generados:\n",
      "   âœ“ Modelo: 316.25 KB\n",
      "   âœ“ Metadata con categorÃ­as limpias: 2.59 KB\n",
      "   âœ“ EstadÃ­sticas: 0.37 KB\n",
      "   âœ“ Datos ejemplo: 11.43 KB\n",
      "\n",
      "ğŸ“Š CategorÃ­as incluidas:\n",
      "\n",
      "   pais_origen: 8 categorÃ­as\n",
      "      â€¢ Bolivia\n",
      "      â€¢ Brasil\n",
      "      â€¢ Chile\n",
      "      â€¢ EEUU, CanadÃ¡ y MÃ©xico\n",
      "      â€¢ Europa y Resto del Mundo\n",
      "      â€¢ Paraguay\n",
      "      â€¢ Resto de AmÃ©rica\n",
      "      â€¢ Uruguay\n",
      "\n",
      "   punto_entrada: 5 categorÃ­as\n",
      "      â€¢ Aeropuerto Buenos Aires\n",
      "      â€¢ Aeropuerto CÃ³rdoba\n",
      "      â€¢ Aeropuerto Mendoza\n",
      "      â€¢ Paso Cristo Redentor\n",
      "      â€¢ Puerto Buenos Aires\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\"\"\"\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "CELDA 9: EXPORTACIÃ“N DEL MODELO PARA PRODUCCIÃ“N (VERSIÃ“N CORREGIDA)\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "Objetivo: Serializar el modelo entrenado con categorÃ­as limpias\n",
    "VersiÃ³n: 2.0 - Sin dependencias de variables faltantes\n",
    "Autor: Juliccc\n",
    "Fecha: 2025-11-05\n",
    "\"\"\"\n",
    "\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ’¾ EXPORTACIÃ“N DEL MODELO PARA PRODUCCIÃ“N (v2.0)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CREAR DIRECTORIO DE MODELOS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\nğŸ“ Creando estructura de directorios...\")\n",
    "\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "    print(\"âœ… Directorio 'models/' creado\")\n",
    "else:\n",
    "    print(\"âœ… Directorio 'models/' ya existe\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# GUARDAR PIPELINE COMPLETO\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\nğŸ’¾ Guardando pipeline del modelo...\")\n",
    "\n",
    "model_filename = 'models/modelo_turismo_mendoza_final.pkl'\n",
    "joblib.dump(final_pipeline, model_filename, compress=3)\n",
    "\n",
    "print(f\"âœ… Modelo guardado en: {model_filename}\")\n",
    "print(f\"   TamaÃ±o: {os.path.getsize(model_filename) / 1024:.2f} KB\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# EXTRAER CATEGORÃAS ÃšNICAS LIMPIAS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\nğŸ” Extrayendo categorÃ­as Ãºnicas del dataset limpio...\")\n",
    "\n",
    "categorias_limpias = {}\n",
    "\n",
    "for col in categorical_features:\n",
    "    if col in df.columns:\n",
    "        valores_unicos = sorted(df[col].dropna().unique().tolist())\n",
    "        categorias_limpias[col] = valores_unicos\n",
    "        \n",
    "        print(f\"\\n   ğŸ“Š {col}: {len(valores_unicos)} categorÃ­as\")\n",
    "        for i, cat in enumerate(valores_unicos, 1):\n",
    "            count = (df[col] == cat).sum()\n",
    "            pct = count / len(df) * 100\n",
    "            print(f\"      {i}. {cat:<45} ({count:>5} registros, {pct:>5.2f}%)\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CALCULAR MÃ‰TRICAS FINALES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\nğŸ“Š Calculando mÃ©tricas finales...\")\n",
    "\n",
    "# Predicciones\n",
    "y_pred_train = final_pipeline.predict(X_train)\n",
    "y_pred_test = final_pipeline.predict(X_test)\n",
    "\n",
    "# MÃ©tricas train\n",
    "train_rmse_final = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "train_r2_final = r2_score(y_train, y_pred_train)\n",
    "\n",
    "# MÃ©tricas test\n",
    "test_rmse_final = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "test_mae_final = mean_absolute_error(y_test, y_pred_test)\n",
    "test_r2_final = r2_score(y_test, y_pred_test)\n",
    "\n",
    "# Overfitting ratio\n",
    "overfitting_ratio_final = test_rmse_final / train_rmse_final\n",
    "\n",
    "print(f\"   Train RMSE: {train_rmse_final:,.0f}\")\n",
    "print(f\"   Test RMSE: {test_rmse_final:,.0f}\")\n",
    "print(f\"   Test MAE: {test_mae_final:,.0f}\")\n",
    "print(f\"   Test RÂ²: {test_r2_final:.4f}\")\n",
    "print(f\"   Overfitting ratio: {overfitting_ratio_final:.2f}x\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# GUARDAR METADATOS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\nğŸ“‹ Guardando metadatos del modelo...\")\n",
    "\n",
    "metadata = {\n",
    "    \"version\": \"2.0\",\n",
    "    \n",
    "    \"model_info\": {\n",
    "        \"nombre\": best_model_name if 'best_model_name' in locals() else \"Gradient Boosting\",\n",
    "        \"tipo\": \"regresion\",\n",
    "        \"framework\": \"scikit-learn\",\n",
    "        \"version_sklearn\": \"1.3.0\",\n",
    "        \"descripcion\": \"Modelo de predicciÃ³n de turistas en Mendoza con categorÃ­as normalizadas\"\n",
    "    },\n",
    "    \n",
    "    \"metricas\": {\n",
    "        \"test_rmse\": float(test_rmse_final),\n",
    "        \"test_mae\": float(test_mae_final),\n",
    "        \"test_r2\": float(test_r2_final),\n",
    "        \"train_rmse\": float(train_rmse_final),\n",
    "        \"train_r2\": float(train_r2_final),\n",
    "        \"overfitting_ratio\": float(overfitting_ratio_final)\n",
    "    },\n",
    "    \n",
    "    \"dataset_info\": {\n",
    "        \"n_registros_train\": int(len(X_train)),\n",
    "        \"n_registros_test\": int(len(X_test)),\n",
    "        \"n_registros_total\": int(len(df)),\n",
    "        \"n_features_input\": int(len(numeric_features) + len(categorical_features)),\n",
    "        \"fecha_corte_train_test\": \"Split temporal 80/20\",\n",
    "        \"periodo_datos\": {\n",
    "            \"inicio\": f\"{int(df['aÃ±o'].min())}-{int(df['mes'].min()):02d}\",\n",
    "            \"fin\": f\"{int(df['aÃ±o'].max())}-{int(df['mes'].max()):02d}\"\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"features\": {\n",
    "        \"numericas\": numeric_features,\n",
    "        \"categoricas\": categorical_features,\n",
    "        \"categorias_unicas\": categorias_limpias\n",
    "    },\n",
    "    \n",
    "    \"preprocessing\": {\n",
    "        \"numeric_imputer\": \"median\",\n",
    "        \"numeric_scaler\": \"StandardScaler\",\n",
    "        \"categorical_imputer\": \"constant (missing)\",\n",
    "        \"categorical_encoder\": \"OneHotEncoder\"\n",
    "    },\n",
    "    \n",
    "    \"training_info\": {\n",
    "        \"fecha_entrenamiento\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"autor\": \"Juliccc\",\n",
    "        \"entrega\": \"4 - VisualizaciÃ³n e IntegraciÃ³n\",\n",
    "        \"random_state\": RANDOM_STATE if 'RANDOM_STATE' in locals() else 42\n",
    "    },\n",
    "    \n",
    "    \"limpieza_datos\": {\n",
    "        \"categorias_normalizadas\": True,\n",
    "        \"mapeos_aplicados\": {\n",
    "            \"pais_origen\": {\n",
    "                \"EEUU variantes\": \"EEUU, CanadÃ¡ y MÃ©xico\",\n",
    "                \"Europa variantes\": \"Europa y Resto del Mundo\",\n",
    "                \"Resto AmÃ©rica variantes\": \"Resto de AmÃ©rica\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_filename = 'models/modelo_metadata.json'\n",
    "with open(metadata_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ… Metadatos guardados en: {metadata_filename}\")\n",
    "print(f\"   TamaÃ±o: {os.path.getsize(metadata_filename) / 1024:.2f} KB\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# GUARDAR ESTADÃSTICAS DEL TARGET\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\nğŸ“Š Guardando estadÃ­sticas del target...\")\n",
    "\n",
    "target_stats = {\n",
    "    \"train\": {\n",
    "        \"mean\": float(y_train.mean()),\n",
    "        \"std\": float(y_train.std()),\n",
    "        \"min\": float(y_train.min()),\n",
    "        \"max\": float(y_train.max()),\n",
    "        \"median\": float(y_train.median()),\n",
    "        \"q25\": float(y_train.quantile(0.25)),\n",
    "        \"q75\": float(y_train.quantile(0.75))\n",
    "    },\n",
    "    \"test\": {\n",
    "        \"mean\": float(y_test.mean()),\n",
    "        \"std\": float(y_test.std()),\n",
    "        \"min\": float(y_test.min()),\n",
    "        \"max\": float(y_test.max()),\n",
    "        \"median\": float(y_test.median()),\n",
    "        \"q25\": float(y_test.quantile(0.25)),\n",
    "        \"q75\": float(y_test.quantile(0.75))\n",
    "    }\n",
    "}\n",
    "\n",
    "stats_filename = 'models/target_stats.json'\n",
    "with open(stats_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(target_stats, f, indent=2)\n",
    "\n",
    "print(f\"âœ… EstadÃ­sticas guardadas en: {stats_filename}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# VERIFICACIÃ“N\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\nğŸ§ª Verificando archivos...\")\n",
    "\n",
    "try:\n",
    "    # Cargar modelo\n",
    "    modelo_test = joblib.load(model_filename)\n",
    "    pred_test = modelo_test.predict(X_test.iloc[:1])\n",
    "    print(f\"âœ… Modelo carga correctamente (predicciÃ³n test: {pred_test[0]:,.0f})\")\n",
    "    \n",
    "    # Verificar metadata\n",
    "    with open(metadata_filename, 'r', encoding='utf-8') as f:\n",
    "        meta_test = json.load(f)\n",
    "    \n",
    "    if 'categorias_unicas' in meta_test['features']:\n",
    "        print(f\"âœ… Metadata con categorÃ­as limpias verificado\")\n",
    "        for col, cats in meta_test['features']['categorias_unicas'].items():\n",
    "            print(f\"   â€¢ {col}: {len(cats)} categorÃ­as\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ ADVERTENCIA: Falta 'categorias_unicas' en metadata\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error en verificaciÃ³n: {str(e)}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# GUARDAR DATOS DE EJEMPLO\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\nğŸ“ Guardando datos de ejemplo...\")\n",
    "\n",
    "sample_data = df.sample(min(100, len(df)), random_state=42)[\n",
    "    numeric_features + categorical_features + ['turistas']\n",
    "]\n",
    "\n",
    "sample_filename = 'models/sample_data.csv'\n",
    "sample_data.to_csv(sample_filename, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"âœ… Datos de ejemplo guardados: {sample_filename}\")\n",
    "print(f\"   {len(sample_data)} registros\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# RESUMEN FINAL\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… EXPORTACIÃ“N COMPLETADA (VERSIÃ“N 2.0)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nğŸ“¦ Archivos generados:\")\n",
    "archivos = [\n",
    "    (model_filename, \"Modelo\"),\n",
    "    (metadata_filename, \"Metadata con categorÃ­as limpias\"),\n",
    "    (stats_filename, \"EstadÃ­sticas\"),\n",
    "    (sample_filename, \"Datos ejemplo\")\n",
    "]\n",
    "\n",
    "for archivo, desc in archivos:\n",
    "    size_kb = os.path.getsize(archivo) / 1024\n",
    "    print(f\"   âœ“ {desc}: {size_kb:.2f} KB\")\n",
    "\n",
    "print(f\"\\nğŸ“Š CategorÃ­as incluidas:\")\n",
    "for col, cats in categorias_limpias.items():\n",
    "    print(f\"\\n   {col}: {len(cats)} categorÃ­as\")\n",
    "    for cat in cats:\n",
    "        print(f\"      â€¢ {cat}\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907cbde1-6e68-4754-a9ba-df64985da794",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
